[Time] - SPS.PSM: 0.0020 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.690185546875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9717610677083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.97119140625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9717610677083334
sparsity Attention k_chunks: 0.96875
sparsity Attention v_chunks: 0.97119140625
[Time] - Reshape: 285090.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9717610677083334
sparsity Attention postReshape k_chunks: 0.96875
sparsity Attention postReshape v_chunks: 0.97119140625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 187566.0000 nanoseconds
sparsity Attention out: 0.9759114583333334
[Time] - Reshape Time-Space: 23754.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9947916666666666
sparsity Attention postReshape chunk - 1 output: 0.9811197916666666
sparsity Attention postReshape chunk - 2 output: 0.9690755208333334
sparsity Attention postReshape chunk - 3 output: 0.9586588541666666
[Time] - Transpose and LIF: 336281.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6885579427083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.97052001953125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6787109375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968994140625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9718424479166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9688313802083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968994140625
sparsity Attention k_chunks: 0.9718424479166666
sparsity Attention v_chunks: 0.9688313802083334
[Time] - Reshape: 298155.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.968994140625
sparsity Attention postReshape k_chunks: 0.9718424479166666
sparsity Attention postReshape v_chunks: 0.9688313802083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 124131.0000 nanoseconds
sparsity Attention out: 0.9783528645833334
[Time] - Reshape Time-Space: 21371.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9801432291666666
sparsity Attention postReshape chunk - 1 output: 0.9837239583333334
sparsity Attention postReshape chunk - 2 output: 0.9752604166666666
sparsity Attention postReshape chunk - 3 output: 0.9742838541666666
[Time] - Transpose and LIF: 330271.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6734212239583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9689534505208334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.667236328125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969970703125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96728515625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9681803385416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969970703125
sparsity Attention k_chunks: 0.96728515625
sparsity Attention v_chunks: 0.9681803385416666
[Time] - Reshape: 291811.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969970703125
sparsity Attention postReshape k_chunks: 0.96728515625
sparsity Attention postReshape v_chunks: 0.9681803385416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 114538.0000 nanoseconds
sparsity Attention out: 0.9761555989583334
[Time] - Reshape Time-Space: 21487.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9860026041666666
sparsity Attention postReshape chunk - 1 output: 0.9899088541666666
sparsity Attention postReshape chunk - 2 output: 0.96484375
sparsity Attention postReshape chunk - 3 output: 0.9638671875
[Time] - Transpose and LIF: 316975.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6663411458333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9678751627604166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6671549479166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676920572916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969970703125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9690755208333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676920572916666
sparsity Attention k_chunks: 0.969970703125
sparsity Attention v_chunks: 0.9690755208333334
[Time] - Reshape: 286139.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9676920572916666
sparsity Attention postReshape k_chunks: 0.969970703125
sparsity Attention postReshape v_chunks: 0.9690755208333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 113276.0000 nanoseconds
sparsity Attention out: 0.9785970052083334
[Time] - Reshape Time-Space: 22149.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9752604166666666
sparsity Attention postReshape chunk - 1 output: 0.9840494791666666
sparsity Attention postReshape chunk - 2 output: 0.9765625
sparsity Attention postReshape chunk - 3 output: 0.978515625
[Time] - Transpose and LIF: 322367.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.662353515625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9681396484375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.65576171875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966064453125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9673665364583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966064453125
sparsity Attention k_chunks: 0.96875
sparsity Attention v_chunks: 0.9673665364583334
[Time] - Reshape: 282511.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.966064453125
sparsity Attention postReshape k_chunks: 0.96875
sparsity Attention postReshape v_chunks: 0.9673665364583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 108377.0000 nanoseconds
sparsity Attention out: 0.9763997395833334
[Time] - Reshape Time-Space: 21246.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9879557291666666
sparsity Attention postReshape chunk - 1 output: 0.9856770833333334
sparsity Attention postReshape chunk - 2 output: 0.9677734375
sparsity Attention postReshape chunk - 3 output: 0.9641927083333334
[Time] - Transpose and LIF: 334280.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9964192708333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6588541666666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676717122395834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6541341145833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9691569010416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967041015625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968505859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9691569010416666
sparsity Attention k_chunks: 0.967041015625
sparsity Attention v_chunks: 0.968505859375
[Time] - Reshape: 279345.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9691569010416666
sparsity Attention postReshape k_chunks: 0.967041015625
sparsity Attention postReshape v_chunks: 0.968505859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107092.0000 nanoseconds
sparsity Attention out: 0.980224609375
[Time] - Reshape Time-Space: 20450.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.98828125
sparsity Attention postReshape chunk - 1 output: 0.9765625
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9840494791666666
[Time] - Transpose and LIF: 318506.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6553548177083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9681396484375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6519368489583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9691569010416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9657389322916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966064453125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9691569010416666
sparsity Attention k_chunks: 0.9657389322916666
sparsity Attention v_chunks: 0.966064453125
[Time] - Reshape: 275706.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9691569010416666
sparsity Attention postReshape k_chunks: 0.9657389322916666
sparsity Attention postReshape v_chunks: 0.966064453125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107254.0000 nanoseconds
sparsity Attention out: 0.9757486979166666
[Time] - Reshape Time-Space: 20131.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9807942708333334
sparsity Attention postReshape chunk - 1 output: 0.9765625
sparsity Attention postReshape chunk - 2 output: 0.9671223958333334
sparsity Attention postReshape chunk - 3 output: 0.978515625
[Time] - Transpose and LIF: 310889.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9934895833333334
sparsity Attention postReshape chunk - 3 x: 0.9944661458333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0035 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.64794921875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676513671875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.65087890625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9674479166666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967041015625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9684244791666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9674479166666666
sparsity Attention k_chunks: 0.967041015625
sparsity Attention v_chunks: 0.9684244791666666
[Time] - Reshape: 282819.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9674479166666666
sparsity Attention postReshape k_chunks: 0.967041015625
sparsity Attention postReshape v_chunks: 0.9684244791666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 104137.0000 nanoseconds
sparsity Attention out: 0.9759928385416666
[Time] - Reshape Time-Space: 20276.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9807942708333334
sparsity Attention postReshape chunk - 1 output: 0.9840494791666666
sparsity Attention postReshape chunk - 2 output: 0.9713541666666666
sparsity Attention postReshape chunk - 3 output: 0.9677734375
[Time] - Transpose and LIF: 317535.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6475423177083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9669596354166666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
[Time] - Spikeformer Time time: 0.0458 seconds
Run 1 - Tempo totale del modello: 0.0459 secondi
[Time] - SPS.PSM: 0.0014 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6825358072916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9710286458333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.97314453125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9694010416666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9710286458333334
sparsity Attention k_chunks: 0.97314453125
sparsity Attention v_chunks: 0.9694010416666666
[Time] - Reshape: 287405.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9710286458333334
sparsity Attention postReshape k_chunks: 0.97314453125
sparsity Attention postReshape v_chunks: 0.9694010416666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 105949.0000 nanoseconds
sparsity Attention out: 0.9811197916666666
[Time] - Reshape Time-Space: 19602.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9918619791666666
sparsity Attention postReshape chunk - 1 output: 0.9957682291666666
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.96484375
[Time] - Transpose and LIF: 324267.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9957682291666666
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.683349609375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9716593424479166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6702473958333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9698079427083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698079427083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9703776041666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9698079427083334
sparsity Attention k_chunks: 0.9698079427083334
sparsity Attention v_chunks: 0.9703776041666666
[Time] - Reshape: 278859.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9698079427083334
sparsity Attention postReshape k_chunks: 0.9698079427083334
sparsity Attention postReshape v_chunks: 0.9703776041666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 104200.0000 nanoseconds
sparsity Attention out: 0.9794108072916666
[Time] - Reshape Time-Space: 19253.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9873046875
sparsity Attention postReshape chunk - 1 output: 0.9847005208333334
sparsity Attention postReshape chunk - 2 output: 0.9759114583333334
sparsity Attention postReshape chunk - 3 output: 0.9697265625
[Time] - Transpose and LIF: 325443.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0035 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6695963541666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9704182942708334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6661783854166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9685872395833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9695638020833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96923828125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9685872395833334
sparsity Attention k_chunks: 0.9695638020833334
sparsity Attention v_chunks: 0.96923828125
[Time] - Reshape: 277850.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9685872395833334
sparsity Attention postReshape k_chunks: 0.9695638020833334
sparsity Attention postReshape v_chunks: 0.96923828125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 103557.0000 nanoseconds
sparsity Attention out: 0.9768880208333334
[Time] - Reshape Time-Space: 18186.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.982421875
sparsity Attention postReshape chunk - 1 output: 0.9915364583333334
sparsity Attention postReshape chunk - 2 output: 0.9641927083333334
sparsity Attention postReshape chunk - 3 output: 0.9694010416666666
[Time] - Transpose and LIF: 301370.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0035 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6658528645833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9684041341145834s
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0015 seconds
sparsity x_for_qkv: 0.6600748697916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.97021484375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969970703125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9689127604166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.97021484375
sparsity Attention k_chunks: 0.969970703125
sparsity Attention v_chunks: 0.9689127604166666
[Time] - Reshape: 278694.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.97021484375
sparsity Attention postReshape k_chunks: 0.969970703125
sparsity Attention postReshape v_chunks: 0.9689127604166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 138794.0000 nanoseconds
sparsity Attention out: 0.9785970052083334
[Time] - Reshape Time-Space: 19482.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9912109375
sparsity Attention postReshape chunk - 1 output: 0.9873046875
sparsity Attention postReshape chunk - 2 output: 0.962890625
sparsity Attention postReshape chunk - 3 output: 0.9729817708333334
[Time] - Transpose and LIF: 317294.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9964192708333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0044 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6625162760416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96893310546875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6575520833333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9688313802083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9679361979166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698079427083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9688313802083334
sparsity Attention k_chunks: 0.9679361979166666
sparsity Attention v_chunks: 0.9698079427083334
[Time] - Reshape: 280069.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9688313802083334
sparsity Attention postReshape k_chunks: 0.9679361979166666
sparsity Attention postReshape v_chunks: 0.9698079427083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107859.0000 nanoseconds
sparsity Attention out: 0.978515625
[Time] - Reshape Time-Space: 19596.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9830729166666666
sparsity Attention postReshape chunk - 1 output: 0.978515625
sparsity Attention postReshape chunk - 2 output: 0.97265625
sparsity Attention postReshape chunk - 3 output: 0.9798177083333334
[Time] - Transpose and LIF: 319141.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6577962239583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9675699869791666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6561686197916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9683430989583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.970458984375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9683430989583334
sparsity Attention k_chunks: 0.96875
sparsity Attention v_chunks: 0.970458984375
[Time] - Reshape: 276403.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9683430989583334
sparsity Attention postReshape k_chunks: 0.96875
sparsity Attention postReshape v_chunks: 0.970458984375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 105863.0000 nanoseconds
sparsity Attention out: 0.9788411458333334
[Time] - Reshape Time-Space: 19283.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9853515625
sparsity Attention postReshape chunk - 1 output: 0.9791666666666666
sparsity Attention postReshape chunk - 2 output: 0.9661458333333334
sparsity Attention postReshape chunk - 3 output: 0.9847005208333334
[Time] - Transpose and LIF: 312288.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9970703125
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.658935546875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9680379231770834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6531575520833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9671223958333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676920572916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96533203125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9671223958333334
sparsity Attention k_chunks: 0.9676920572916666
sparsity Attention v_chunks: 0.96533203125
[Time] - Reshape: 272594.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9671223958333334
sparsity Attention postReshape k_chunks: 0.9676920572916666
sparsity Attention postReshape v_chunks: 0.96533203125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 103728.0000 nanoseconds
sparsity Attention out: 0.9725748697916666
[Time] - Reshape Time-Space: 22008.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.97265625
sparsity Attention postReshape chunk - 1 output: 0.9778645833333334
sparsity Attention postReshape chunk - 2 output: 0.9710286458333334
sparsity Attention postReshape chunk - 3 output: 0.96875
[Time] - Transpose and LIF: 317801.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.99609375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6525065104166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9669392903645834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6509602864583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966552734375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.965576171875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9658203125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966552734375
sparsity Attention k_chunks: 0.965576171875
sparsity Attention v_chunks: 0.9658203125
[Time] - Reshape: 282853.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.966552734375
sparsity Attention postReshape k_chunks: 0.965576171875
sparsity Attention postReshape v_chunks: 0.9658203125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 108991.0000 nanoseconds
sparsity Attention out: 0.9707845052083334
[Time] - Reshape Time-Space: 18508.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9794921875
sparsity Attention postReshape chunk - 1 output: 0.970703125
sparsity Attention postReshape chunk - 2 output: 0.9654947916666666
sparsity Attention postReshape chunk - 3 output: 0.9674479166666666
[Time] - Transpose and LIF: 312046.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9973958333333334
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0035 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6460774739583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9683634440104166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
[Time] - Spikeformer Time time: 0.0445 seconds
Run 2 - Tempo totale del modello: 0.0445 secondi
[Time] - SPS.PSM: 0.0028 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.7032063802083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9706217447916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9702962239583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9706217447916666
sparsity Attention k_chunks: 0.9702962239583334
sparsity Attention v_chunks: 0.969970703125
[Time] - Reshape: 282477.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9706217447916666
sparsity Attention postReshape k_chunks: 0.9702962239583334
sparsity Attention postReshape v_chunks: 0.969970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 266316.0000 nanoseconds
sparsity Attention out: 0.9739583333333334
[Time] - Reshape Time-Space: 25094.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9938151041666666
sparsity Attention postReshape chunk - 1 output: 0.9921875
sparsity Attention postReshape chunk - 2 output: 0.9469401041666666
sparsity Attention postReshape chunk - 3 output: 0.962890625
[Time] - Transpose and LIF: 361130.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9964192708333334
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.7056477864583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.97021484375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6979166666666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9705403645833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9685872395833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9694010416666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9705403645833334
sparsity Attention k_chunks: 0.9685872395833334
sparsity Attention v_chunks: 0.9694010416666666
[Time] - Reshape: 255080.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9705403645833334
sparsity Attention postReshape k_chunks: 0.9685872395833334
sparsity Attention postReshape v_chunks: 0.9694010416666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 281087.0000 nanoseconds
sparsity Attention out: 0.9777018229166666
[Time] - Reshape Time-Space: 27420.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.998046875
sparsity Attention postReshape chunk - 1 output: 0.9886067708333334
sparsity Attention postReshape chunk - 2 output: 0.9641927083333334
sparsity Attention postReshape chunk - 3 output: 0.9599609375
[Time] - Transpose and LIF: 363090.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 0.9944661458333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6834309895833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9691569010416666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.679443359375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9713541666666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970458984375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9703776041666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9713541666666666
sparsity Attention k_chunks: 0.970458984375
sparsity Attention v_chunks: 0.9703776041666666
[Time] - Reshape: 256792.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9713541666666666
sparsity Attention postReshape k_chunks: 0.970458984375
sparsity Attention postReshape v_chunks: 0.9703776041666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 102472.0000 nanoseconds
sparsity Attention out: 0.9765625
[Time] - Reshape Time-Space: 18291.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9873046875
sparsity Attention postReshape chunk - 1 output: 0.9889322916666666
sparsity Attention postReshape chunk - 2 output: 0.9690755208333334
sparsity Attention postReshape chunk - 3 output: 0.9609375
[Time] - Transpose and LIF: 441603.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6771647135416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96783447265625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6729329427083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96826171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676920572916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698893229166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96826171875
sparsity Attention k_chunks: 0.9676920572916666
sparsity Attention v_chunks: 0.9698893229166666
[Time] - Reshape: 282668.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96826171875
sparsity Attention postReshape k_chunks: 0.9676920572916666
sparsity Attention postReshape v_chunks: 0.9698893229166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 259288.0000 nanoseconds
sparsity Attention out: 0.9729817708333334
[Time] - Reshape Time-Space: 24357.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9873046875
sparsity Attention postReshape chunk - 1 output: 0.9915364583333334
sparsity Attention postReshape chunk - 2 output: 0.9501953125
sparsity Attention postReshape chunk - 3 output: 0.962890625
[Time] - Transpose and LIF: 367114.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9938151041666666
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.66943359375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96771240234375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.66455078125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9679361979166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9678548177083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9681803385416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9679361979166666
sparsity Attention k_chunks: 0.9678548177083334
sparsity Attention v_chunks: 0.9681803385416666
[Time] - Reshape: 258404.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9679361979166666
sparsity Attention postReshape k_chunks: 0.9678548177083334
sparsity Attention postReshape v_chunks: 0.9681803385416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 264800.0000 nanoseconds
sparsity Attention out: 0.97265625
[Time] - Reshape Time-Space: 25238.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9915364583333334
sparsity Attention postReshape chunk - 1 output: 0.9720052083333334
sparsity Attention postReshape chunk - 2 output: 0.96484375
sparsity Attention postReshape chunk - 3 output: 0.9622395833333334
[Time] - Transpose and LIF: 377641.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9964192708333334
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6649576822916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9667765299479166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6595865885416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.964111328125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9673665364583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9669596354166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.964111328125
sparsity Attention k_chunks: 0.9673665364583334
sparsity Attention v_chunks: 0.9669596354166666
[Time] - Reshape: 255530.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.964111328125
sparsity Attention postReshape k_chunks: 0.9673665364583334
sparsity Attention postReshape v_chunks: 0.9669596354166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 103063.0000 nanoseconds
sparsity Attention out: 0.9688313802083334
[Time] - Reshape Time-Space: 18845.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9723307291666666
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.966796875
sparsity Attention postReshape chunk - 3 output: 0.9560546875
[Time] - Transpose and LIF: 332582.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.9973958333333334
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6555989583333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9668172200520834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6529134114583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96630859375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9669596354166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9677734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96630859375
sparsity Attention k_chunks: 0.9669596354166666
sparsity Attention v_chunks: 0.9677734375
[Time] - Reshape: 259170.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96630859375
sparsity Attention postReshape k_chunks: 0.9669596354166666
sparsity Attention postReshape v_chunks: 0.9677734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 271044.0000 nanoseconds
sparsity Attention out: 0.9698893229166666
[Time] - Reshape Time-Space: 25788.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9798177083333334
sparsity Attention postReshape chunk - 1 output: 0.986328125
sparsity Attention postReshape chunk - 2 output: 0.962890625
sparsity Attention postReshape chunk - 3 output: 0.9505208333333334
[Time] - Transpose and LIF: 366820.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.9931640625
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.65478515625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9682413736979166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.652587890625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9679361979166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9656575520833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9663899739583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9679361979166666
sparsity Attention k_chunks: 0.9656575520833334
sparsity Attention v_chunks: 0.9663899739583334
[Time] - Reshape: 250652.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9679361979166666
sparsity Attention postReshape k_chunks: 0.9656575520833334
sparsity Attention postReshape v_chunks: 0.9663899739583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 250312.0000 nanoseconds
sparsity Attention out: 0.9737141927083334
[Time] - Reshape Time-Space: 25768.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9860026041666666
sparsity Attention postReshape chunk - 1 output: 0.9680989583333334
sparsity Attention postReshape chunk - 2 output: 0.9723307291666666
sparsity Attention postReshape chunk - 3 output: 0.9684244791666666
[Time] - Transpose and LIF: 345189.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.65087890625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9674275716145834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
[Time] - Spikeformer Time time: 0.0492 seconds
Run 3 - Tempo totale del modello: 0.0492 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.685302734375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9713541666666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9716796875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9701334635416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9713541666666666
sparsity Attention k_chunks: 0.9716796875
sparsity Attention v_chunks: 0.9701334635416666
[Time] - Reshape: 266234.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9713541666666666
sparsity Attention postReshape k_chunks: 0.9716796875
sparsity Attention postReshape v_chunks: 0.9701334635416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 104627.0000 nanoseconds
sparsity Attention out: 0.9808756510416666
[Time] - Reshape Time-Space: 19392.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9905598958333334
sparsity Attention postReshape chunk - 1 output: 0.9905598958333334
sparsity Attention postReshape chunk - 2 output: 0.9739583333333334
sparsity Attention postReshape chunk - 3 output: 0.9684244791666666
[Time] - Transpose and LIF: 353685.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.689453125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9687703450520834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6787923177083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9696451822916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9677734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.97021484375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9696451822916666
sparsity Attention k_chunks: 0.9677734375
sparsity Attention v_chunks: 0.97021484375
[Time] - Reshape: 250858.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9696451822916666
sparsity Attention postReshape k_chunks: 0.9677734375
sparsity Attention postReshape v_chunks: 0.97021484375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 103422.0000 nanoseconds
sparsity Attention out: 0.9751790364583334
[Time] - Reshape Time-Space: 19571.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9869791666666666
sparsity Attention postReshape chunk - 1 output: 0.9814453125
sparsity Attention postReshape chunk - 2 output: 0.9593098958333334
sparsity Attention postReshape chunk - 3 output: 0.9729817708333334
[Time] - Transpose and LIF: 354868.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9964192708333334
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6790364583333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9695638020833334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6731770833333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9673665364583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968994140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9659830729166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9673665364583334
sparsity Attention k_chunks: 0.968994140625
sparsity Attention v_chunks: 0.9659830729166666
[Time] - Reshape: 265651.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9673665364583334
sparsity Attention postReshape k_chunks: 0.968994140625
sparsity Attention postReshape v_chunks: 0.9659830729166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 264133.0000 nanoseconds
sparsity Attention out: 0.975830078125
[Time] - Reshape Time-Space: 25600.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.984375
sparsity Attention postReshape chunk - 1 output: 0.9860026041666666
sparsity Attention postReshape chunk - 2 output: 0.966796875
sparsity Attention postReshape chunk - 3 output: 0.9661458333333334
[Time] - Transpose and LIF: 371827.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.669677734375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679768880208334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6665852864583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96826171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9677734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9671223958333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96826171875
sparsity Attention k_chunks: 0.9677734375
sparsity Attention v_chunks: 0.9671223958333334
[Time] - Reshape: 255207.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96826171875
sparsity Attention postReshape k_chunks: 0.9677734375
sparsity Attention postReshape v_chunks: 0.9671223958333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 253353.0000 nanoseconds
sparsity Attention out: 0.9723307291666666
[Time] - Reshape Time-Space: 24458.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9827473958333334
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.9684244791666666
sparsity Attention postReshape chunk - 3 output: 0.9580078125
[Time] - Transpose and LIF: 364352.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.663330078125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96832275390625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6586100260416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968505859375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969482421875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96533203125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968505859375
sparsity Attention k_chunks: 0.969482421875
sparsity Attention v_chunks: 0.96533203125
[Time] - Reshape: 247449.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.968505859375
sparsity Attention postReshape k_chunks: 0.969482421875
sparsity Attention postReshape v_chunks: 0.96533203125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 105285.0000 nanoseconds
sparsity Attention out: 0.975341796875
[Time] - Reshape Time-Space: 18830.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9765625
sparsity Attention postReshape chunk - 1 output: 0.9765625
sparsity Attention postReshape chunk - 2 output: 0.978515625
sparsity Attention postReshape chunk - 3 output: 0.9697265625
[Time] - Transpose and LIF: 337185.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0036 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.659912109375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.968505859375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6555989583333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96728515625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9683430989583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9694010416666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96728515625
sparsity Attention k_chunks: 0.9683430989583334
sparsity Attention v_chunks: 0.9694010416666666
[Time] - Reshape: 249863.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96728515625
sparsity Attention postReshape k_chunks: 0.9683430989583334
sparsity Attention postReshape v_chunks: 0.9694010416666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 256262.0000 nanoseconds
sparsity Attention out: 0.9761555989583334
[Time] - Reshape Time-Space: 25756.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9827473958333334
sparsity Attention postReshape chunk - 1 output: 0.9736328125
sparsity Attention postReshape chunk - 2 output: 0.96875
sparsity Attention postReshape chunk - 3 output: 0.9794921875
[Time] - Transpose and LIF: 370181.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.65576171875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676310221354166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6505533854166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9688313802083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9691569010416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96728515625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9688313802083334
sparsity Attention k_chunks: 0.9691569010416666
sparsity Attention v_chunks: 0.96728515625
[Time] - Reshape: 250177.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9688313802083334
sparsity Attention postReshape k_chunks: 0.9691569010416666
sparsity Attention postReshape v_chunks: 0.96728515625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 259374.0000 nanoseconds
sparsity Attention out: 0.97705078125
[Time] - Reshape Time-Space: 25165.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9869791666666666
sparsity Attention postReshape chunk - 1 output: 0.9814453125
sparsity Attention postReshape chunk - 2 output: 0.97265625
sparsity Attention postReshape chunk - 3 output: 0.9671223958333334
[Time] - Transpose and LIF: 371433.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9970703125
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6473795572916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9674479166666666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6444498697916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9642740885416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969970703125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9662272135416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9642740885416666
sparsity Attention k_chunks: 0.969970703125
sparsity Attention v_chunks: 0.9662272135416666
[Time] - Reshape: 249339.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9642740885416666
sparsity Attention postReshape k_chunks: 0.969970703125
sparsity Attention postReshape v_chunks: 0.9662272135416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 109890.0000 nanoseconds
sparsity Attention out: 0.9755859375
[Time] - Reshape Time-Space: 18804.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.982421875
sparsity Attention postReshape chunk - 1 output: 0.9811197916666666
sparsity Attention postReshape chunk - 2 output: 0.9716796875
sparsity Attention postReshape chunk - 3 output: 0.9671223958333334
[Time] - Transpose and LIF: 359971.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9957682291666666
sparsity Attention postReshape chunk - 3 x: 0.9957682291666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6456705729166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9667561848958334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
[Time] - Spikeformer Time time: 0.0478 seconds
Run 4 - Tempo totale del modello: 0.0478 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6787923177083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9695638020833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9735514322916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676920572916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9695638020833334
sparsity Attention k_chunks: 0.9735514322916666
sparsity Attention v_chunks: 0.9676920572916666
[Time] - Reshape: 251863.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9695638020833334
sparsity Attention postReshape k_chunks: 0.9735514322916666
sparsity Attention postReshape v_chunks: 0.9676920572916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 103405.0000 nanoseconds
sparsity Attention out: 0.9798177083333334
[Time] - Reshape Time-Space: 18920.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9895833333333334
sparsity Attention postReshape chunk - 1 output: 0.9879557291666666
sparsity Attention postReshape chunk - 2 output: 0.966796875
sparsity Attention postReshape chunk - 3 output: 0.9749348958333334
[Time] - Transpose and LIF: 343876.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6752115885416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9687906901041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.665771484375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9673665364583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9657389322916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9689127604166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9673665364583334
sparsity Attention k_chunks: 0.9657389322916666
sparsity Attention v_chunks: 0.9689127604166666
[Time] - Reshape: 258444.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9673665364583334
sparsity Attention postReshape k_chunks: 0.9657389322916666
sparsity Attention postReshape v_chunks: 0.9689127604166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 260090.0000 nanoseconds
sparsity Attention out: 0.9729817708333334
[Time] - Reshape Time-Space: 26546.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.98046875
sparsity Attention postReshape chunk - 1 output: 0.9759114583333334
sparsity Attention postReshape chunk - 2 output: 0.9729817708333334
sparsity Attention postReshape chunk - 3 output: 0.9625651041666666
[Time] - Transpose and LIF: 360074.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6634114583333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9683634440104166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6539713541666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96923828125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9683430989583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9683430989583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96923828125
sparsity Attention k_chunks: 0.9683430989583334
sparsity Attention v_chunks: 0.9683430989583334
[Time] - Reshape: 249713.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96923828125
sparsity Attention postReshape k_chunks: 0.9683430989583334
sparsity Attention postReshape v_chunks: 0.9683430989583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 262521.0000 nanoseconds
sparsity Attention out: 0.9756673177083334
[Time] - Reshape Time-Space: 25282.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9899088541666666
sparsity Attention postReshape chunk - 1 output: 0.9768880208333334
sparsity Attention postReshape chunk - 2 output: 0.9661458333333334
sparsity Attention postReshape chunk - 3 output: 0.9697265625
[Time] - Transpose and LIF: 369233.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.65478515625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676310221354166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6549479166666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9677734375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9662272135416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9669596354166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9677734375
sparsity Attention k_chunks: 0.9662272135416666
sparsity Attention v_chunks: 0.9669596354166666
[Time] - Reshape: 249983.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9677734375
sparsity Attention postReshape k_chunks: 0.9662272135416666
sparsity Attention postReshape v_chunks: 0.9669596354166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 104802.0000 nanoseconds
sparsity Attention out: 0.9713541666666666
[Time] - Reshape Time-Space: 18750.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.978515625
sparsity Attention postReshape chunk - 1 output: 0.9606119791666666
sparsity Attention postReshape chunk - 2 output: 0.9742838541666666
sparsity Attention postReshape chunk - 3 output: 0.9720052083333334
[Time] - Transpose and LIF: 360901.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9951171875
sparsity Attention postReshape chunk - 2 x: 0.9957682291666666
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6534830729166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9671630859375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6512858072916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9658203125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96630859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9679361979166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9658203125
sparsity Attention k_chunks: 0.96630859375
sparsity Attention v_chunks: 0.9679361979166666
[Time] - Reshape: 267304.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9658203125
sparsity Attention postReshape k_chunks: 0.96630859375
sparsity Attention postReshape v_chunks: 0.9679361979166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 254723.0000 nanoseconds
sparsity Attention out: 0.9740397135416666
[Time] - Reshape Time-Space: 25234.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9781901041666666
sparsity Attention postReshape chunk - 1 output: 0.9778645833333334
sparsity Attention postReshape chunk - 2 output: 0.9664713541666666
sparsity Attention postReshape chunk - 3 output: 0.9736328125
[Time] - Transpose and LIF: 370241.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.99609375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.647216796875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9659016927083334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6474609375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9654947916666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9683430989583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9674479166666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9654947916666666
sparsity Attention k_chunks: 0.9683430989583334
sparsity Attention v_chunks: 0.9674479166666666
[Time] - Reshape: 252690.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9654947916666666
sparsity Attention postReshape k_chunks: 0.9683430989583334
sparsity Attention postReshape v_chunks: 0.9674479166666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 249488.0000 nanoseconds
sparsity Attention out: 0.9724934895833334
[Time] - Reshape Time-Space: 24018.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9739583333333334
sparsity Attention postReshape chunk - 1 output: 0.9723307291666666
sparsity Attention postReshape chunk - 2 output: 0.9778645833333334
sparsity Attention postReshape chunk - 3 output: 0.9658203125
[Time] - Transpose and LIF: 354526.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.644287109375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9663492838541666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.64453125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96826171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967041015625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9654947916666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96826171875
sparsity Attention k_chunks: 0.967041015625
sparsity Attention v_chunks: 0.9654947916666666
[Time] - Reshape: 251368.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96826171875
sparsity Attention postReshape k_chunks: 0.967041015625
sparsity Attention postReshape v_chunks: 0.9654947916666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 104798.0000 nanoseconds
sparsity Attention out: 0.9749348958333334
[Time] - Reshape Time-Space: 19492.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.978515625
sparsity Attention postReshape chunk - 1 output: 0.9814453125
sparsity Attention postReshape chunk - 2 output: 0.9677734375
sparsity Attention postReshape chunk - 3 output: 0.9720052083333334
[Time] - Transpose and LIF: 355216.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9938151041666666
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6424967447916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9662679036458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6442057291666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9658203125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96435546875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96630859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9658203125
sparsity Attention k_chunks: 0.96435546875
sparsity Attention v_chunks: 0.96630859375
[Time] - Reshape: 255188.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9658203125
sparsity Attention postReshape k_chunks: 0.96435546875
sparsity Attention postReshape v_chunks: 0.96630859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 298080.0000 nanoseconds
sparsity Attention out: 0.9732259114583334
[Time] - Reshape Time-Space: 28541.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9762369791666666
sparsity Attention postReshape chunk - 1 output: 0.97265625
sparsity Attention postReshape chunk - 2 output: 0.9729817708333334
sparsity Attention postReshape chunk - 3 output: 0.9710286458333334
[Time] - Transpose and LIF: 375769.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6416829427083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9673258463541666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
[Time] - Spikeformer Time time: 0.0488 seconds
Run 5 - Tempo totale del modello: 0.0488 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.7044270833333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9691569010416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9697265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968994140625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9691569010416666
sparsity Attention k_chunks: 0.9697265625
sparsity Attention v_chunks: 0.968994140625
[Time] - Reshape: 266370.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9691569010416666
sparsity Attention postReshape k_chunks: 0.9697265625
sparsity Attention postReshape v_chunks: 0.968994140625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 263896.0000 nanoseconds
sparsity Attention out: 0.96533203125
[Time] - Reshape Time-Space: 27568.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9925130208333334
sparsity Attention postReshape chunk - 1 output: 0.9833984375
sparsity Attention postReshape chunk - 2 output: 0.943359375
sparsity Attention postReshape chunk - 3 output: 0.9420572916666666
[Time] - Transpose and LIF: 374230.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9944661458333334
sparsity Attention postReshape chunk - 3 x: 0.9964192708333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.694091796875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9695841471354166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.682861328125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.97021484375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9702962239583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9701334635416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.97021484375
sparsity Attention k_chunks: 0.9702962239583334
sparsity Attention v_chunks: 0.9701334635416666
[Time] - Reshape: 249944.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.97021484375
sparsity Attention postReshape k_chunks: 0.9702962239583334
sparsity Attention postReshape v_chunks: 0.9701334635416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 258091.0000 nanoseconds
sparsity Attention out: 0.9749348958333334
[Time] - Reshape Time-Space: 26271.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9954427083333334
sparsity Attention postReshape chunk - 1 output: 0.9918619791666666
sparsity Attention postReshape chunk - 2 output: 0.95703125
sparsity Attention postReshape chunk - 3 output: 0.9554036458333334
[Time] - Transpose and LIF: 457311.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9934895833333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6822916666666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96728515625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6773274739583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9685872395833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969482421875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9693196614583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9685872395833334
sparsity Attention k_chunks: 0.969482421875
sparsity Attention v_chunks: 0.9693196614583334
[Time] - Reshape: 253179.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9685872395833334
sparsity Attention postReshape k_chunks: 0.969482421875
sparsity Attention postReshape v_chunks: 0.9693196614583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 105767.0000 nanoseconds
sparsity Attention out: 0.9722493489583334
[Time] - Reshape Time-Space: 19857.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9908854166666666
sparsity Attention postReshape chunk - 1 output: 0.98828125
sparsity Attention postReshape chunk - 2 output: 0.9554036458333334
sparsity Attention postReshape chunk - 3 output: 0.9544270833333334
[Time] - Transpose and LIF: 347951.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9954427083333334
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6754557291666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.967529296875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.668701171875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676106770833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966552734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9691569010416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676106770833334
sparsity Attention k_chunks: 0.966552734375
sparsity Attention v_chunks: 0.9691569010416666
[Time] - Reshape: 264221.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9676106770833334
sparsity Attention postReshape k_chunks: 0.966552734375
sparsity Attention postReshape v_chunks: 0.9691569010416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 257579.0000 nanoseconds
sparsity Attention out: 0.9725748697916666
[Time] - Reshape Time-Space: 25881.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9837239583333334
sparsity Attention postReshape chunk - 1 output: 0.9899088541666666
sparsity Attention postReshape chunk - 2 output: 0.9560546875
sparsity Attention postReshape chunk - 3 output: 0.9606119791666666
[Time] - Transpose and LIF: 378508.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9964192708333334
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6678873697916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96771240234375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6630045572916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9674479166666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698079427083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9668782552083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9674479166666666
sparsity Attention k_chunks: 0.9698079427083334
sparsity Attention v_chunks: 0.9668782552083334
[Time] - Reshape: 251436.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9674479166666666
sparsity Attention postReshape k_chunks: 0.9698079427083334
sparsity Attention postReshape v_chunks: 0.9668782552083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 273404.0000 nanoseconds
sparsity Attention out: 0.9727376302083334
[Time] - Reshape Time-Space: 26060.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9856770833333334
sparsity Attention postReshape chunk - 1 output: 0.9794921875
sparsity Attention postReshape chunk - 2 output: 0.9625651041666666
sparsity Attention postReshape chunk - 3 output: 0.9632161458333334
[Time] - Transpose and LIF: 373015.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9970703125
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6627604166666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96728515625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.657958984375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9688313802083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9715983072916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9674479166666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9688313802083334
sparsity Attention k_chunks: 0.9715983072916666
sparsity Attention v_chunks: 0.9674479166666666
[Time] - Reshape: 251607.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9688313802083334
sparsity Attention postReshape k_chunks: 0.9715983072916666
sparsity Attention postReshape v_chunks: 0.9674479166666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 251907.0000 nanoseconds
sparsity Attention out: 0.9785970052083334
[Time] - Reshape Time-Space: 25231.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9814453125
sparsity Attention postReshape chunk - 1 output: 0.9866536458333334
sparsity Attention postReshape chunk - 2 output: 0.974609375
sparsity Attention postReshape chunk - 3 output: 0.9716796875
[Time] - Transpose and LIF: 355935.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6625162760416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9688924153645834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6599934895833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676920572916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9684244791666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9674479166666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676920572916666
sparsity Attention k_chunks: 0.9684244791666666
sparsity Attention v_chunks: 0.9674479166666666
[Time] - Reshape: 261741.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9676920572916666
sparsity Attention postReshape k_chunks: 0.9684244791666666
sparsity Attention postReshape v_chunks: 0.9674479166666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 103058.0000 nanoseconds
sparsity Attention out: 0.9710286458333334
[Time] - Reshape Time-Space: 18068.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9850260416666666
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.9567057291666666
sparsity Attention postReshape chunk - 3 output: 0.9619140625
[Time] - Transpose and LIF: 351036.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.654052734375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9669189453125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6512044270833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9659830729166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9685872395833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9659830729166666
sparsity Attention k_chunks: 0.9685872395833334
sparsity Attention v_chunks: 0.96875
[Time] - Reshape: 251041.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9659830729166666
sparsity Attention postReshape k_chunks: 0.9685872395833334
sparsity Attention postReshape v_chunks: 0.96875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 255144.0000 nanoseconds
sparsity Attention out: 0.9711100260416666
[Time] - Reshape Time-Space: 25337.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9794921875
sparsity Attention postReshape chunk - 1 output: 0.9752604166666666
sparsity Attention postReshape chunk - 2 output: 0.9671223958333334
sparsity Attention postReshape chunk - 3 output: 0.9625651041666666
[Time] - Transpose and LIF: 361810.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.647216796875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9670613606770834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
[Time] - Spikeformer Time time: 0.0481 seconds
Run 6 - Tempo totale del modello: 0.0481 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6892903645833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9717610677083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9718424479166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9717610677083334
sparsity Attention k_chunks: 0.9718424479166666
sparsity Attention v_chunks: 0.96875
[Time] - Reshape: 255935.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9717610677083334
sparsity Attention postReshape k_chunks: 0.9718424479166666
sparsity Attention postReshape v_chunks: 0.96875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 266971.0000 nanoseconds
sparsity Attention out: 0.9774576822916666
[Time] - Reshape Time-Space: 27166.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9928385416666666
sparsity Attention postReshape chunk - 1 output: 0.9886067708333334
sparsity Attention postReshape chunk - 2 output: 0.9498697916666666
sparsity Attention postReshape chunk - 3 output: 0.978515625
[Time] - Transpose and LIF: 374618.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.99609375
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6943359375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.97113037109375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6824544270833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.97119140625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676920572916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9693196614583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.97119140625
sparsity Attention k_chunks: 0.9676920572916666
sparsity Attention v_chunks: 0.9693196614583334
[Time] - Reshape: 256478.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.97119140625
sparsity Attention postReshape k_chunks: 0.9676920572916666
sparsity Attention postReshape v_chunks: 0.9693196614583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 272512.0000 nanoseconds
sparsity Attention out: 0.9795735677083334
[Time] - Reshape Time-Space: 27063.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.994140625
sparsity Attention postReshape chunk - 1 output: 0.9908854166666666
sparsity Attention postReshape chunk - 2 output: 0.9622395833333334
sparsity Attention postReshape chunk - 3 output: 0.9710286458333334
[Time] - Transpose and LIF: 372260.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.99609375
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6832682291666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.97027587890625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6792805989583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9701334635416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9707845052083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968505859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9701334635416666
sparsity Attention k_chunks: 0.9707845052083334
sparsity Attention v_chunks: 0.968505859375
[Time] - Reshape: 267324.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9701334635416666
sparsity Attention postReshape k_chunks: 0.9707845052083334
sparsity Attention postReshape v_chunks: 0.968505859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 112696.0000 nanoseconds
sparsity Attention out: 0.9759114583333334
[Time] - Reshape Time-Space: 19829.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9915364583333334
sparsity Attention postReshape chunk - 1 output: 0.9827473958333334
sparsity Attention postReshape chunk - 2 output: 0.9684244791666666
sparsity Attention postReshape chunk - 3 output: 0.9609375
[Time] - Transpose and LIF: 350691.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6739908854166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9698893229166666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6671549479166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9686686197916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9685872395833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9685872395833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9686686197916666
sparsity Attention k_chunks: 0.9685872395833334
sparsity Attention v_chunks: 0.9685872395833334
[Time] - Reshape: 275349.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9686686197916666
sparsity Attention postReshape k_chunks: 0.9685872395833334
sparsity Attention postReshape v_chunks: 0.9685872395833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 253581.0000 nanoseconds
sparsity Attention out: 0.9750162760416666
[Time] - Reshape Time-Space: 31438.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9827473958333334
sparsity Attention postReshape chunk - 1 output: 0.9811197916666666
sparsity Attention postReshape chunk - 2 output: 0.9742838541666666
sparsity Attention postReshape chunk - 3 output: 0.9619140625
[Time] - Transpose and LIF: 360963.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6669108072916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96734619140625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6607259114583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968017578125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96826171875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9708658854166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968017578125
sparsity Attention k_chunks: 0.96826171875
sparsity Attention v_chunks: 0.9708658854166666
[Time] - Reshape: 252467.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.968017578125
sparsity Attention postReshape k_chunks: 0.96826171875
sparsity Attention postReshape v_chunks: 0.9708658854166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 103756.0000 nanoseconds
sparsity Attention out: 0.9767252604166666
[Time] - Reshape Time-Space: 18347.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9856770833333334
sparsity Attention postReshape chunk - 1 output: 0.9856770833333334
sparsity Attention postReshape chunk - 2 output: 0.9733072916666666
sparsity Attention postReshape chunk - 3 output: 0.9622395833333334
[Time] - Transpose and LIF: 349293.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.994140625
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6590983072916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9685872395833334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6581217447916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9678548177083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966064453125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9689127604166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9678548177083334
sparsity Attention k_chunks: 0.966064453125
sparsity Attention v_chunks: 0.9689127604166666
[Time] - Reshape: 258882.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9678548177083334
sparsity Attention postReshape k_chunks: 0.966064453125
sparsity Attention postReshape v_chunks: 0.9689127604166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 104631.0000 nanoseconds
sparsity Attention out: 0.9784342447916666
[Time] - Reshape Time-Space: 19831.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9801432291666666
sparsity Attention postReshape chunk - 1 output: 0.9860026041666666
sparsity Attention postReshape chunk - 2 output: 0.9749348958333334
sparsity Attention postReshape chunk - 3 output: 0.97265625
[Time] - Transpose and LIF: 357267.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.656494140625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9678141276041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.65087890625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9671223958333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9686686197916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9685872395833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9671223958333334
sparsity Attention k_chunks: 0.9686686197916666
sparsity Attention v_chunks: 0.9685872395833334
[Time] - Reshape: 254494.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9671223958333334
sparsity Attention postReshape k_chunks: 0.9686686197916666
sparsity Attention postReshape v_chunks: 0.9685872395833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 260314.0000 nanoseconds
sparsity Attention out: 0.978515625
[Time] - Reshape Time-Space: 25697.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9837239583333334
sparsity Attention postReshape chunk - 1 output: 0.9856770833333334
sparsity Attention postReshape chunk - 2 output: 0.98046875
sparsity Attention postReshape chunk - 3 output: 0.9641927083333334
[Time] - Transpose and LIF: 381352.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.65087890625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9685465494791666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.651123046875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.970458984375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969482421875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9684244791666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.970458984375
sparsity Attention k_chunks: 0.969482421875
sparsity Attention v_chunks: 0.9684244791666666
[Time] - Reshape: 278528.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.970458984375
sparsity Attention postReshape k_chunks: 0.969482421875
sparsity Attention postReshape v_chunks: 0.9684244791666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 252318.0000 nanoseconds
sparsity Attention out: 0.9775390625
[Time] - Reshape Time-Space: 24096.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9840494791666666
sparsity Attention postReshape chunk - 1 output: 0.9833984375
sparsity Attention postReshape chunk - 2 output: 0.9703776041666666
sparsity Attention postReshape chunk - 3 output: 0.9723307291666666
[Time] - Transpose and LIF: 351100.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6504720052083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9685262044270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
[Time] - Spikeformer Time time: 0.0491 seconds
Run 7 - Tempo totale del modello: 0.0491 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.686767578125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.97216796875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.972900390625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.97216796875
sparsity Attention k_chunks: 0.972900390625
sparsity Attention v_chunks: 0.969970703125
[Time] - Reshape: 253902.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.97216796875
sparsity Attention postReshape k_chunks: 0.972900390625
sparsity Attention postReshape v_chunks: 0.969970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 117358.0000 nanoseconds
sparsity Attention out: 0.9859212239583334
[Time] - Reshape Time-Space: 20412.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9957682291666666
sparsity Attention postReshape chunk - 1 output: 0.9925130208333334
sparsity Attention postReshape chunk - 2 output: 0.9814453125
sparsity Attention postReshape chunk - 3 output: 0.9739583333333334
[Time] - Transpose and LIF: 344184.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9964192708333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6888020833333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9715983072916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.677734375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698079427083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96875
sparsity Attention k_chunks: 0.9698079427083334
sparsity Attention v_chunks: 0.970703125
[Time] - Reshape: 254598.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96875
sparsity Attention postReshape k_chunks: 0.9698079427083334
sparsity Attention postReshape v_chunks: 0.970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 104332.0000 nanoseconds
sparsity Attention out: 0.9768880208333334
[Time] - Reshape Time-Space: 20358.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9889322916666666
sparsity Attention postReshape chunk - 1 output: 0.9853515625
sparsity Attention postReshape chunk - 2 output: 0.9651692708333334
sparsity Attention postReshape chunk - 3 output: 0.9680989583333334
[Time] - Transpose and LIF: 358811.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6787109375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9701131184895834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.670166015625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.970947265625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9679361979166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9694010416666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.970947265625
sparsity Attention k_chunks: 0.9679361979166666
sparsity Attention v_chunks: 0.9694010416666666
[Time] - Reshape: 253142.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.970947265625
sparsity Attention postReshape k_chunks: 0.9679361979166666
sparsity Attention postReshape v_chunks: 0.9694010416666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 249173.0000 nanoseconds
sparsity Attention out: 0.9759928385416666
[Time] - Reshape Time-Space: 27479.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9915364583333334
sparsity Attention postReshape chunk - 1 output: 0.9866536458333334
sparsity Attention postReshape chunk - 2 output: 0.9677734375
sparsity Attention postReshape chunk - 3 output: 0.9580078125
[Time] - Transpose and LIF: 377261.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6681315104166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9687906901041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6617024739583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9684244791666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9668782552083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9659016927083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9684244791666666
sparsity Attention k_chunks: 0.9668782552083334
sparsity Attention v_chunks: 0.9659016927083334
[Time] - Reshape: 251197.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9684244791666666
sparsity Attention postReshape k_chunks: 0.9668782552083334
sparsity Attention postReshape v_chunks: 0.9659016927083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 261884.0000 nanoseconds
sparsity Attention out: 0.9696451822916666
[Time] - Reshape Time-Space: 25440.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.970703125
sparsity Attention postReshape chunk - 1 output: 0.9772135416666666
sparsity Attention postReshape chunk - 2 output: 0.9690755208333334
sparsity Attention postReshape chunk - 3 output: 0.9615885416666666
[Time] - Transpose and LIF: 356727.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.994140625
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6599934895833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9688313802083334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.65625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969482421875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968994140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96826171875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969482421875
sparsity Attention k_chunks: 0.968994140625
sparsity Attention v_chunks: 0.96826171875
[Time] - Reshape: 256348.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969482421875
sparsity Attention postReshape k_chunks: 0.968994140625
sparsity Attention postReshape v_chunks: 0.96826171875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 104149.0000 nanoseconds
sparsity Attention out: 0.978515625
[Time] - Reshape Time-Space: 18703.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9830729166666666
sparsity Attention postReshape chunk - 1 output: 0.9830729166666666
sparsity Attention postReshape chunk - 2 output: 0.9762369791666666
sparsity Attention postReshape chunk - 3 output: 0.9716796875
[Time] - Transpose and LIF: 356966.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6529134114583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9673055013020834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6531575520833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968505859375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9684244791666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968994140625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968505859375
sparsity Attention k_chunks: 0.9684244791666666
sparsity Attention v_chunks: 0.968994140625
[Time] - Reshape: 264576.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.968505859375
sparsity Attention postReshape k_chunks: 0.9684244791666666
sparsity Attention postReshape v_chunks: 0.968994140625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 265316.0000 nanoseconds
sparsity Attention out: 0.9813639322916666
[Time] - Reshape Time-Space: 25575.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9840494791666666
sparsity Attention postReshape chunk - 1 output: 0.9853515625
sparsity Attention postReshape chunk - 2 output: 0.9811197916666666
sparsity Attention postReshape chunk - 3 output: 0.9749348958333334
[Time] - Transpose and LIF: 363792.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6495768229166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.967529296875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6515299479166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96484375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9677734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9656575520833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96484375
sparsity Attention k_chunks: 0.9677734375
sparsity Attention v_chunks: 0.9656575520833334
[Time] - Reshape: 273086.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96484375
sparsity Attention postReshape k_chunks: 0.9677734375
sparsity Attention postReshape v_chunks: 0.9656575520833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 249565.0000 nanoseconds
sparsity Attention out: 0.970703125
[Time] - Reshape Time-Space: 23577.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9710286458333334
sparsity Attention postReshape chunk - 1 output: 0.966796875
sparsity Attention postReshape chunk - 2 output: 0.9739583333333334
sparsity Attention postReshape chunk - 3 output: 0.9710286458333334
[Time] - Transpose and LIF: 351338.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.9964192708333334
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9951171875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6481119791666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9674886067708334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.644287109375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9673665364583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9658203125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9686686197916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9673665364583334
sparsity Attention k_chunks: 0.9658203125
sparsity Attention v_chunks: 0.9686686197916666
[Time] - Reshape: 280187.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9673665364583334
sparsity Attention postReshape k_chunks: 0.9658203125
sparsity Attention postReshape v_chunks: 0.9686686197916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 101400.0000 nanoseconds
sparsity Attention out: 0.977294921875
[Time] - Reshape Time-Space: 19174.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9840494791666666
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.9664713541666666
sparsity Attention postReshape chunk - 3 output: 0.9781901041666666
[Time] - Transpose and LIF: 356805.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6437174479166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9663289388020834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
[Time] - Spikeformer Time time: 0.0483 seconds
Run 8 - Tempo totale del modello: 0.0483 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6715494791666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9706217447916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9727376302083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9703776041666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9706217447916666
sparsity Attention k_chunks: 0.9727376302083334
sparsity Attention v_chunks: 0.9703776041666666
[Time] - Reshape: 257639.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9706217447916666
sparsity Attention postReshape k_chunks: 0.9727376302083334
sparsity Attention postReshape v_chunks: 0.9703776041666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 103544.0000 nanoseconds
sparsity Attention out: 0.9807942708333334
[Time] - Reshape Time-Space: 19949.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9869791666666666
sparsity Attention postReshape chunk - 1 output: 0.9856770833333334
sparsity Attention postReshape chunk - 2 output: 0.9664713541666666
sparsity Attention postReshape chunk - 3 output: 0.9840494791666666
[Time] - Transpose and LIF: 356647.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6781412760416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9714762369791666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6671549479166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9684244791666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.971923828125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9684244791666666
sparsity Attention k_chunks: 0.96875
sparsity Attention v_chunks: 0.971923828125
[Time] - Reshape: 266228.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9684244791666666
sparsity Attention postReshape k_chunks: 0.96875
sparsity Attention postReshape v_chunks: 0.971923828125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 256131.0000 nanoseconds
sparsity Attention out: 0.974365234375
[Time] - Reshape Time-Space: 25488.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9866536458333334
sparsity Attention postReshape chunk - 1 output: 0.9759114583333334
sparsity Attention postReshape chunk - 2 output: 0.9710286458333334
sparsity Attention postReshape chunk - 3 output: 0.9638671875
[Time] - Transpose and LIF: 378110.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6639811197916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9682413736979166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.65869140625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9703776041666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9689127604166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968994140625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9703776041666666
sparsity Attention k_chunks: 0.9689127604166666
sparsity Attention v_chunks: 0.968994140625
[Time] - Reshape: 251995.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9703776041666666
sparsity Attention postReshape k_chunks: 0.9689127604166666
sparsity Attention postReshape v_chunks: 0.968994140625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 247913.0000 nanoseconds
sparsity Attention out: 0.9750162760416666
[Time] - Reshape Time-Space: 24930.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9778645833333334
sparsity Attention postReshape chunk - 1 output: 0.9794921875
sparsity Attention postReshape chunk - 2 output: 0.9736328125
sparsity Attention postReshape chunk - 3 output: 0.9690755208333334
[Time] - Transpose and LIF: 356817.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9967447916666666
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6556803385416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9686686197916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6513671875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9673665364583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9705403645833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969482421875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9673665364583334
sparsity Attention k_chunks: 0.9705403645833334
sparsity Attention v_chunks: 0.969482421875
[Time] - Reshape: 254469.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9673665364583334
sparsity Attention postReshape k_chunks: 0.9705403645833334
sparsity Attention postReshape v_chunks: 0.969482421875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 104763.0000 nanoseconds
sparsity Attention out: 0.9769694010416666
[Time] - Reshape Time-Space: 26612.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9794921875
sparsity Attention postReshape chunk - 1 output: 0.9788411458333334
sparsity Attention postReshape chunk - 2 output: 0.982421875
sparsity Attention postReshape chunk - 3 output: 0.9671223958333334
[Time] - Transpose and LIF: 353139.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9970703125
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6555989583333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679158528645834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6495768229166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9654134114583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9683430989583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966064453125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9654134114583334
sparsity Attention k_chunks: 0.9683430989583334
sparsity Attention v_chunks: 0.966064453125
[Time] - Reshape: 256622.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9654134114583334
sparsity Attention postReshape k_chunks: 0.9683430989583334
sparsity Attention postReshape v_chunks: 0.966064453125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 256827.0000 nanoseconds
sparsity Attention out: 0.9793294270833334
[Time] - Reshape Time-Space: 24782.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9847005208333334
sparsity Attention postReshape chunk - 1 output: 0.9830729166666666
sparsity Attention postReshape chunk - 2 output: 0.9781901041666666
sparsity Attention postReshape chunk - 3 output: 0.9713541666666666
[Time] - Transpose and LIF: 375366.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9944661458333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.647216796875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9664103190104166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6481119791666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9671223958333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969482421875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676106770833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9671223958333334
sparsity Attention k_chunks: 0.969482421875
sparsity Attention v_chunks: 0.9676106770833334
[Time] - Reshape: 263557.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9671223958333334
sparsity Attention postReshape k_chunks: 0.969482421875
sparsity Attention postReshape v_chunks: 0.9676106770833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 247719.0000 nanoseconds
sparsity Attention out: 0.97265625
[Time] - Reshape Time-Space: 24304.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.970703125
sparsity Attention postReshape chunk - 1 output: 0.9700520833333334
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.9729817708333334
[Time] - Transpose and LIF: 383011.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.99609375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6458333333333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9682210286458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6438802083333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676106770833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9639485677083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9690755208333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676106770833334
sparsity Attention k_chunks: 0.9639485677083334
sparsity Attention v_chunks: 0.9690755208333334
[Time] - Reshape: 252309.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9676106770833334
sparsity Attention postReshape k_chunks: 0.9639485677083334
sparsity Attention postReshape v_chunks: 0.9690755208333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 104778.0000 nanoseconds
sparsity Attention out: 0.9727376302083334
[Time] - Reshape Time-Space: 22986.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9762369791666666
sparsity Attention postReshape chunk - 1 output: 0.9817708333333334
sparsity Attention postReshape chunk - 2 output: 0.9759114583333334
sparsity Attention postReshape chunk - 3 output: 0.95703125
[Time] - Transpose and LIF: 352956.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6410319010416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9670206705729166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6398111979166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969970703125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9685872395833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968017578125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969970703125
sparsity Attention k_chunks: 0.9685872395833334
sparsity Attention v_chunks: 0.968017578125
[Time] - Reshape: 255167.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969970703125
sparsity Attention postReshape k_chunks: 0.9685872395833334
sparsity Attention postReshape v_chunks: 0.968017578125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 114759.0000 nanoseconds
sparsity Attention out: 0.97705078125
[Time] - Reshape Time-Space: 19946.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.974609375
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.9759114583333334
sparsity Attention postReshape chunk - 3 output: 0.9775390625
[Time] - Transpose and LIF: 343308.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6385904947916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9674886067708334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
[Time] - Spikeformer Time time: 0.0487 seconds
Run 9 - Tempo totale del modello: 0.0487 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.68212890625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9703776041666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9703776041666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.97216796875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9703776041666666
sparsity Attention k_chunks: 0.9703776041666666
sparsity Attention v_chunks: 0.97216796875
[Time] - Reshape: 256362.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9703776041666666
sparsity Attention postReshape k_chunks: 0.9703776041666666
sparsity Attention postReshape v_chunks: 0.97216796875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 265909.0000 nanoseconds
sparsity Attention out: 0.9765625
[Time] - Reshape Time-Space: 26207.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.986328125
sparsity Attention postReshape chunk - 1 output: 0.9921875
sparsity Attention postReshape chunk - 2 output: 0.9788411458333334
sparsity Attention postReshape chunk - 3 output: 0.9488932291666666
[Time] - Transpose and LIF: 360411.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9954427083333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.67041015625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9703572591145834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6693522135416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9689127604166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9686686197916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9684244791666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9689127604166666
sparsity Attention k_chunks: 0.9686686197916666
sparsity Attention v_chunks: 0.9684244791666666
[Time] - Reshape: 253722.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9689127604166666
sparsity Attention postReshape k_chunks: 0.9686686197916666
sparsity Attention postReshape v_chunks: 0.9684244791666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 253072.0000 nanoseconds
sparsity Attention out: 0.97412109375
[Time] - Reshape Time-Space: 26695.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9775390625
sparsity Attention postReshape chunk - 1 output: 0.9817708333333334
sparsity Attention postReshape chunk - 2 output: 0.9694010416666666
sparsity Attention postReshape chunk - 3 output: 0.9677734375
[Time] - Transpose and LIF: 368473.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9944661458333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.672607421875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9698079427083334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.664794921875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9706217447916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698893229166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9706217447916666
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.9698893229166666
[Time] - Reshape: 257438.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9706217447916666
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.9698893229166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 105687.0000 nanoseconds
sparsity Attention out: 0.9771321614583334
[Time] - Reshape Time-Space: 18957.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9840494791666666
sparsity Attention postReshape chunk - 1 output: 0.9853515625
sparsity Attention postReshape chunk - 2 output: 0.9677734375
sparsity Attention postReshape chunk - 3 output: 0.9713541666666666
[Time] - Transpose and LIF: 360139.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6560872395833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9695027669270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6512858072916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9700520833333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9677734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9701334635416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9700520833333334
sparsity Attention k_chunks: 0.9677734375
sparsity Attention v_chunks: 0.9701334635416666
[Time] - Reshape: 260695.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9700520833333334
sparsity Attention postReshape k_chunks: 0.9677734375
sparsity Attention postReshape v_chunks: 0.9701334635416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 256764.0000 nanoseconds
sparsity Attention out: 0.9781901041666666
[Time] - Reshape Time-Space: 24172.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9860026041666666
sparsity Attention postReshape chunk - 1 output: 0.9850260416666666
sparsity Attention postReshape chunk - 2 output: 0.9690755208333334
sparsity Attention postReshape chunk - 3 output: 0.97265625
[Time] - Transpose and LIF: 365443.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.65087890625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9693196614583334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.65087890625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969970703125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9673665364583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9679361979166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969970703125
sparsity Attention k_chunks: 0.9673665364583334
sparsity Attention v_chunks: 0.9679361979166666
[Time] - Reshape: 255251.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969970703125
sparsity Attention postReshape k_chunks: 0.9673665364583334
sparsity Attention postReshape v_chunks: 0.9679361979166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 265148.0000 nanoseconds
sparsity Attention out: 0.9759114583333334
[Time] - Reshape Time-Space: 26726.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9847005208333334
sparsity Attention postReshape chunk - 1 output: 0.9807942708333334
sparsity Attention postReshape chunk - 2 output: 0.970703125
sparsity Attention postReshape chunk - 3 output: 0.9674479166666666
[Time] - Transpose and LIF: 440204.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.99609375
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.647216796875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9678548177083334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6464029947916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9672037760416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9650065104166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9672037760416666
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.9650065104166666
[Time] - Reshape: 252297.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9672037760416666
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.9650065104166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 103685.0000 nanoseconds
sparsity Attention out: 0.9722493489583334
[Time] - Reshape Time-Space: 19522.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9716796875
sparsity Attention postReshape chunk - 1 output: 0.9775390625
sparsity Attention postReshape chunk - 2 output: 0.9680989583333334
sparsity Attention postReshape chunk - 3 output: 0.9716796875
[Time] - Transpose and LIF: 368304.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9967447916666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.642578125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9689534505208334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6443684895833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9677734375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9674479166666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.965087890625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9677734375
sparsity Attention k_chunks: 0.9674479166666666
sparsity Attention v_chunks: 0.965087890625
[Time] - Reshape: 257661.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9677734375
sparsity Attention postReshape k_chunks: 0.9674479166666666
sparsity Attention postReshape v_chunks: 0.965087890625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 262944.0000 nanoseconds
sparsity Attention out: 0.9696451822916666
[Time] - Reshape Time-Space: 42550.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9772135416666666
sparsity Attention postReshape chunk - 1 output: 0.9739583333333334
sparsity Attention postReshape chunk - 2 output: 0.9645182291666666
sparsity Attention postReshape chunk - 3 output: 0.962890625
[Time] - Transpose and LIF: 440790.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6433919270833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9673055013020834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6421712239583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9666341145833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9668782552083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9678548177083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9666341145833334
sparsity Attention k_chunks: 0.9668782552083334
sparsity Attention v_chunks: 0.9678548177083334
[Time] - Reshape: 273486.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9666341145833334
sparsity Attention postReshape k_chunks: 0.9668782552083334
sparsity Attention postReshape v_chunks: 0.9678548177083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 247206.0000 nanoseconds
sparsity Attention out: 0.9740397135416666
[Time] - Reshape Time-Space: 24233.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9710286458333334
sparsity Attention postReshape chunk - 1 output: 0.9837239583333334
sparsity Attention postReshape chunk - 2 output: 0.9733072916666666
sparsity Attention postReshape chunk - 3 output: 0.9680989583333334
[Time] - Transpose and LIF: 358416.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.99609375
sparsity Attention postReshape chunk - 3 x: 0.9934895833333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6395670572916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9665323893229166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
[Time] - Spikeformer Time time: 0.0487 seconds
Run 10 - Tempo totale del modello: 0.0487 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6826171875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9669596354166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9701334635416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9681803385416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9669596354166666
sparsity Attention k_chunks: 0.9701334635416666
sparsity Attention v_chunks: 0.9681803385416666
[Time] - Reshape: 252731.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9669596354166666
sparsity Attention postReshape k_chunks: 0.9701334635416666
sparsity Attention postReshape v_chunks: 0.9681803385416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 105514.0000 nanoseconds
sparsity Attention out: 0.9763997395833334
[Time] - Reshape Time-Space: 19371.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9912109375
sparsity Attention postReshape chunk - 1 output: 0.9788411458333334
sparsity Attention postReshape chunk - 2 output: 0.9651692708333334
sparsity Attention postReshape chunk - 3 output: 0.9703776041666666
[Time] - Transpose and LIF: 358764.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.9931640625
sparsity Attention postReshape chunk - 3 x: 0.99609375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6840006510416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9685262044270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6746419270833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9677734375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698079427083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96826171875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9677734375
sparsity Attention k_chunks: 0.9698079427083334
sparsity Attention v_chunks: 0.96826171875
[Time] - Reshape: 252519.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9677734375
sparsity Attention postReshape k_chunks: 0.9698079427083334
sparsity Attention postReshape v_chunks: 0.96826171875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 103740.0000 nanoseconds
sparsity Attention out: 0.9769694010416666
[Time] - Reshape Time-Space: 22425.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9947916666666666
sparsity Attention postReshape chunk - 1 output: 0.9798177083333334
sparsity Attention postReshape chunk - 2 output: 0.9645182291666666
sparsity Attention postReshape chunk - 3 output: 0.96875
[Time] - Transpose and LIF: 341787.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6688639322916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9682210286458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6622721354166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9679361979166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9696451822916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966064453125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9679361979166666
sparsity Attention k_chunks: 0.9696451822916666
sparsity Attention v_chunks: 0.966064453125
[Time] - Reshape: 251565.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9679361979166666
sparsity Attention postReshape k_chunks: 0.9696451822916666
sparsity Attention postReshape v_chunks: 0.966064453125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 256105.0000 nanoseconds
sparsity Attention out: 0.9766438802083334
[Time] - Reshape Time-Space: 25313.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9873046875
sparsity Attention postReshape chunk - 1 output: 0.9755859375
sparsity Attention postReshape chunk - 2 output: 0.9716796875
sparsity Attention postReshape chunk - 3 output: 0.9720052083333334
[Time] - Transpose and LIF: 358153.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6590983072916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9686279296875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.656494140625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967529296875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967529296875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966552734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967529296875
sparsity Attention k_chunks: 0.967529296875
sparsity Attention v_chunks: 0.966552734375
[Time] - Reshape: 264533.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.967529296875
sparsity Attention postReshape k_chunks: 0.967529296875
sparsity Attention postReshape v_chunks: 0.966552734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 252688.0000 nanoseconds
sparsity Attention out: 0.9742024739583334
[Time] - Reshape Time-Space: 25411.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9807942708333334
sparsity Attention postReshape chunk - 1 output: 0.9733072916666666
sparsity Attention postReshape chunk - 2 output: 0.9593098958333334
sparsity Attention postReshape chunk - 3 output: 0.9833984375
[Time] - Transpose and LIF: 375918.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6527506510416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9674479166666666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6524251302083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9688313802083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9681803385416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9678548177083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9688313802083334
sparsity Attention k_chunks: 0.9681803385416666
sparsity Attention v_chunks: 0.9678548177083334
[Time] - Reshape: 254931.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9688313802083334
sparsity Attention postReshape k_chunks: 0.9681803385416666
sparsity Attention postReshape v_chunks: 0.9678548177083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 104814.0000 nanoseconds
sparsity Attention out: 0.9781087239583334
[Time] - Reshape Time-Space: 19955.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9853515625
sparsity Attention postReshape chunk - 1 output: 0.9860026041666666
sparsity Attention postReshape chunk - 2 output: 0.9690755208333334
sparsity Attention postReshape chunk - 3 output: 0.9720052083333334
[Time] - Transpose and LIF: 358854.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6537272135416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672037760416666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6490071614583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96826171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.964111328125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9685872395833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96826171875
sparsity Attention k_chunks: 0.964111328125
sparsity Attention v_chunks: 0.9685872395833334
[Time] - Reshape: 263935.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96826171875
sparsity Attention postReshape k_chunks: 0.964111328125
sparsity Attention postReshape v_chunks: 0.9685872395833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 255950.0000 nanoseconds
sparsity Attention out: 0.9754231770833334
[Time] - Reshape Time-Space: 29525.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9820963541666666
sparsity Attention postReshape chunk - 1 output: 0.984375
sparsity Attention postReshape chunk - 2 output: 0.9625651041666666
sparsity Attention postReshape chunk - 3 output: 0.97265625
[Time] - Transpose and LIF: 376223.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.994140625
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0045 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6488444010416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9675496419270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6529134114583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9661458333333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966796875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966064453125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9661458333333334
sparsity Attention k_chunks: 0.966796875
sparsity Attention v_chunks: 0.966064453125
[Time] - Reshape: 253862.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9661458333333334
sparsity Attention postReshape k_chunks: 0.966796875
sparsity Attention postReshape v_chunks: 0.966064453125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 279788.0000 nanoseconds
sparsity Attention out: 0.9666341145833334
[Time] - Reshape Time-Space: 31587.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9697265625
sparsity Attention postReshape chunk - 1 output: 0.9778645833333334
sparsity Attention postReshape chunk - 2 output: 0.9619140625
sparsity Attention postReshape chunk - 3 output: 0.95703125
[Time] - Transpose and LIF: 380108.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9964192708333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.64794921875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9675496419270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.64453125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9680989583333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967041015625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9667154947916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9680989583333334
sparsity Attention k_chunks: 0.967041015625
sparsity Attention v_chunks: 0.9667154947916666
[Time] - Reshape: 252586.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9680989583333334
sparsity Attention postReshape k_chunks: 0.967041015625
sparsity Attention postReshape v_chunks: 0.9667154947916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 114545.0000 nanoseconds
sparsity Attention out: 0.9735514322916666
[Time] - Reshape Time-Space: 20692.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9827473958333334
sparsity Attention postReshape chunk - 1 output: 0.9723307291666666
sparsity Attention postReshape chunk - 2 output: 0.9736328125
sparsity Attention postReshape chunk - 3 output: 0.9654947916666666
[Time] - Transpose and LIF: 388495.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6409505208333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96527099609375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
[Time] - Spikeformer Time time: 0.0489 seconds
Run 11 - Tempo totale del modello: 0.0489 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0005 seconds
sparsity x_for_qkv: 0.6841634114583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9694010416666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9693196614583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96923828125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9694010416666666
sparsity Attention k_chunks: 0.9693196614583334
sparsity Attention v_chunks: 0.96923828125
[Time] - Reshape: 259543.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9694010416666666
sparsity Attention postReshape k_chunks: 0.9693196614583334
sparsity Attention postReshape v_chunks: 0.96923828125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 108598.0000 nanoseconds
sparsity Attention out: 0.9764811197916666
[Time] - Reshape Time-Space: 20753.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9915364583333334
sparsity Attention postReshape chunk - 1 output: 0.9869791666666666
sparsity Attention postReshape chunk - 2 output: 0.9612630208333334
sparsity Attention postReshape chunk - 3 output: 0.9661458333333334
[Time] - Transpose and LIF: 353671.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9912109375
sparsity Attention postReshape chunk - 3 x: 0.9931640625
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.678466796875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9704793294270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6642252604166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9688313802083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9697265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9684244791666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9688313802083334
sparsity Attention k_chunks: 0.9697265625
sparsity Attention v_chunks: 0.9684244791666666
[Time] - Reshape: 252977.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9688313802083334
sparsity Attention postReshape k_chunks: 0.9697265625
sparsity Attention postReshape v_chunks: 0.9684244791666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 255538.0000 nanoseconds
sparsity Attention out: 0.9763997395833334
[Time] - Reshape Time-Space: 24797.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9788411458333334
sparsity Attention postReshape chunk - 1 output: 0.9807942708333334
sparsity Attention postReshape chunk - 2 output: 0.9752604166666666
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 368283.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9964192708333334
sparsity Attention postReshape chunk - 3 x: 0.9964192708333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6608072916666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.968994140625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6551920572916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96728515625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96630859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9697265625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96728515625
sparsity Attention k_chunks: 0.96630859375
sparsity Attention v_chunks: 0.9697265625
[Time] - Reshape: 268131.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96728515625
sparsity Attention postReshape k_chunks: 0.96630859375
sparsity Attention postReshape v_chunks: 0.9697265625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 106027.0000 nanoseconds
sparsity Attention out: 0.973388671875
[Time] - Reshape Time-Space: 19098.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9716796875
sparsity Attention postReshape chunk - 1 output: 0.982421875
sparsity Attention postReshape chunk - 2 output: 0.9694010416666666
sparsity Attention postReshape chunk - 3 output: 0.9700520833333334
[Time] - Transpose and LIF: 369997.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.9973958333333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6512858072916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96826171875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.64892578125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9702962239583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9689127604166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9710286458333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9702962239583334
sparsity Attention k_chunks: 0.9689127604166666
sparsity Attention v_chunks: 0.9710286458333334
[Time] - Reshape: 255499.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9702962239583334
sparsity Attention postReshape k_chunks: 0.9689127604166666
sparsity Attention postReshape v_chunks: 0.9710286458333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 105581.0000 nanoseconds
sparsity Attention out: 0.98095703125
[Time] - Reshape Time-Space: 20083.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.986328125
sparsity Attention postReshape chunk - 1 output: 0.9837239583333334
sparsity Attention postReshape chunk - 2 output: 0.9807942708333334
sparsity Attention postReshape chunk - 3 output: 0.9729817708333334
[Time] - Transpose and LIF: 353305.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6472981770833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679768880208334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6439615885416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676106770833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9707845052083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698893229166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676106770833334
sparsity Attention k_chunks: 0.9707845052083334
sparsity Attention v_chunks: 0.9698893229166666
[Time] - Reshape: 251092.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9676106770833334
sparsity Attention postReshape k_chunks: 0.9707845052083334
sparsity Attention postReshape v_chunks: 0.9698893229166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 258763.0000 nanoseconds
sparsity Attention out: 0.9818522135416666
[Time] - Reshape Time-Space: 26006.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9814453125
sparsity Attention postReshape chunk - 1 output: 0.9840494791666666
sparsity Attention postReshape chunk - 2 output: 0.9801432291666666
sparsity Attention postReshape chunk - 3 output: 0.9817708333333334
[Time] - Transpose and LIF: 356146.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.640625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9683430989583334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6416015625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.970458984375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968505859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9667154947916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.970458984375
sparsity Attention k_chunks: 0.968505859375
sparsity Attention v_chunks: 0.9667154947916666
[Time] - Reshape: 256693.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.970458984375
sparsity Attention postReshape k_chunks: 0.968505859375
sparsity Attention postReshape v_chunks: 0.9667154947916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 249779.0000 nanoseconds
sparsity Attention out: 0.97802734375
[Time] - Reshape Time-Space: 25555.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9677734375
sparsity Attention postReshape chunk - 1 output: 0.9827473958333334
sparsity Attention postReshape chunk - 2 output: 0.9749348958333334
sparsity Attention postReshape chunk - 3 output: 0.9866536458333334
[Time] - Transpose and LIF: 382950.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6402994791666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.968505859375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6372884114583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9698893229166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9672037760416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676920572916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9698893229166666
sparsity Attention k_chunks: 0.9672037760416666
sparsity Attention v_chunks: 0.9676920572916666
[Time] - Reshape: 261428.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9698893229166666
sparsity Attention postReshape k_chunks: 0.9672037760416666
sparsity Attention postReshape v_chunks: 0.9676920572916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 103535.0000 nanoseconds
sparsity Attention out: 0.9777018229166666
[Time] - Reshape Time-Space: 19086.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9736328125
sparsity Attention postReshape chunk - 1 output: 0.9807942708333334
sparsity Attention postReshape chunk - 2 output: 0.9860026041666666
sparsity Attention postReshape chunk - 3 output: 0.9703776041666666
[Time] - Transpose and LIF: 357656.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.635986328125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9671630859375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6361490885416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9684244791666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676106770833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9667154947916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9684244791666666
sparsity Attention k_chunks: 0.9676106770833334
sparsity Attention v_chunks: 0.9667154947916666
[Time] - Reshape: 249951.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9684244791666666
sparsity Attention postReshape k_chunks: 0.9676106770833334
sparsity Attention postReshape v_chunks: 0.9667154947916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 267803.0000 nanoseconds
sparsity Attention out: 0.9755045572916666
[Time] - Reshape Time-Space: 25413.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9798177083333334
sparsity Attention postReshape chunk - 1 output: 0.970703125
sparsity Attention postReshape chunk - 2 output: 0.9791666666666666
sparsity Attention postReshape chunk - 3 output: 0.9723307291666666
[Time] - Transpose and LIF: 356120.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6372884114583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9670613606770834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
[Time] - Spikeformer Time time: 0.0483 seconds
Run 12 - Tempo totale del modello: 0.0484 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6878255208333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9733072916666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970947265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9733072916666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9733072916666666
sparsity Attention k_chunks: 0.970947265625
sparsity Attention v_chunks: 0.9733072916666666
[Time] - Reshape: 257525.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9733072916666666
sparsity Attention postReshape k_chunks: 0.970947265625
sparsity Attention postReshape v_chunks: 0.9733072916666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 268888.0000 nanoseconds
sparsity Attention out: 0.981689453125
[Time] - Reshape Time-Space: 25279.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9951171875
sparsity Attention postReshape chunk - 1 output: 0.9918619791666666
sparsity Attention postReshape chunk - 2 output: 0.9752604166666666
sparsity Attention postReshape chunk - 3 output: 0.9645182291666666
[Time] - Transpose and LIF: 444103.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6809895833333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9705607096354166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.67041015625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969482421875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.97119140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9702962239583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969482421875
sparsity Attention k_chunks: 0.97119140625
sparsity Attention v_chunks: 0.9702962239583334
[Time] - Reshape: 255096.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969482421875
sparsity Attention postReshape k_chunks: 0.97119140625
sparsity Attention postReshape v_chunks: 0.9702962239583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 250334.0000 nanoseconds
sparsity Attention out: 0.97998046875
[Time] - Reshape Time-Space: 24436.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9827473958333334
sparsity Attention postReshape chunk - 1 output: 0.9905598958333334
sparsity Attention postReshape chunk - 2 output: 0.9729817708333334
sparsity Attention postReshape chunk - 3 output: 0.9736328125
[Time] - Transpose and LIF: 370202.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9957682291666666
sparsity Attention postReshape chunk - 3 x: 0.99609375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.67236328125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96868896484375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6610514322916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676920572916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9677734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9656575520833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676920572916666
sparsity Attention k_chunks: 0.9677734375
sparsity Attention v_chunks: 0.9656575520833334
[Time] - Reshape: 270530.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9676920572916666
sparsity Attention postReshape k_chunks: 0.9677734375
sparsity Attention postReshape v_chunks: 0.9656575520833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 104124.0000 nanoseconds
sparsity Attention out: 0.9681803385416666
[Time] - Reshape Time-Space: 20412.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9847005208333334
sparsity Attention postReshape chunk - 1 output: 0.9752604166666666
sparsity Attention postReshape chunk - 2 output: 0.943359375
sparsity Attention postReshape chunk - 3 output: 0.9694010416666666
[Time] - Transpose and LIF: 370331.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9970703125
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0037 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.66259765625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9673055013020834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6571451822916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969482421875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9677734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9678548177083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969482421875
sparsity Attention k_chunks: 0.9677734375
sparsity Attention v_chunks: 0.9678548177083334
[Time] - Reshape: 250672.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969482421875
sparsity Attention postReshape k_chunks: 0.9677734375
sparsity Attention postReshape v_chunks: 0.9678548177083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 264720.0000 nanoseconds
sparsity Attention out: 0.9730631510416666
[Time] - Reshape Time-Space: 25982.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9814453125
sparsity Attention postReshape chunk - 1 output: 0.978515625
sparsity Attention postReshape chunk - 2 output: 0.9619140625
sparsity Attention postReshape chunk - 3 output: 0.9703776041666666
[Time] - Transpose and LIF: 365404.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9951171875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6554361979166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679158528645834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.652587890625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9678548177083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9671223958333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9663899739583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9678548177083334
sparsity Attention k_chunks: 0.9671223958333334
sparsity Attention v_chunks: 0.9663899739583334
[Time] - Reshape: 251388.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9678548177083334
sparsity Attention postReshape k_chunks: 0.9671223958333334
sparsity Attention postReshape v_chunks: 0.9663899739583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 249815.0000 nanoseconds
sparsity Attention out: 0.96923828125
[Time] - Reshape Time-Space: 25310.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9856770833333334
sparsity Attention postReshape chunk - 1 output: 0.974609375
sparsity Attention postReshape chunk - 2 output: 0.9664713541666666
sparsity Attention postReshape chunk - 3 output: 0.9501953125
[Time] - Transpose and LIF: 356237.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6513671875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676717122395834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6468098958333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9667154947916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9671223958333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9672037760416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9667154947916666
sparsity Attention k_chunks: 0.9671223958333334
sparsity Attention v_chunks: 0.9672037760416666
[Time] - Reshape: 255588.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9667154947916666
sparsity Attention postReshape k_chunks: 0.9671223958333334
sparsity Attention postReshape v_chunks: 0.9672037760416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 104473.0000 nanoseconds
sparsity Attention out: 0.971923828125
[Time] - Reshape Time-Space: 19443.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.98046875
sparsity Attention postReshape chunk - 1 output: 0.970703125
sparsity Attention postReshape chunk - 2 output: 0.9567057291666666
sparsity Attention postReshape chunk - 3 output: 0.9798177083333334
[Time] - Transpose and LIF: 370637.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6442057291666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9673258463541666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6446940104166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9669596354166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9668782552083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676920572916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9669596354166666
sparsity Attention k_chunks: 0.9668782552083334
sparsity Attention v_chunks: 0.9676920572916666
[Time] - Reshape: 264649.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9669596354166666
sparsity Attention postReshape k_chunks: 0.9668782552083334
sparsity Attention postReshape v_chunks: 0.9676920572916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 261528.0000 nanoseconds
sparsity Attention out: 0.9798177083333334
[Time] - Reshape Time-Space: 25418.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9837239583333334
sparsity Attention postReshape chunk - 1 output: 0.986328125
sparsity Attention postReshape chunk - 2 output: 0.978515625
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 466775.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6455891927083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9664510091145834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6420084635416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9667154947916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9663899739583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9657389322916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9667154947916666
sparsity Attention k_chunks: 0.9663899739583334
sparsity Attention v_chunks: 0.9657389322916666
[Time] - Reshape: 285619.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9667154947916666
sparsity Attention postReshape k_chunks: 0.9663899739583334
sparsity Attention postReshape v_chunks: 0.9657389322916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 250552.0000 nanoseconds
sparsity Attention out: 0.9739583333333334
[Time] - Reshape Time-Space: 24266.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9736328125
sparsity Attention postReshape chunk - 1 output: 0.9781901041666666
sparsity Attention postReshape chunk - 2 output: 0.9736328125
sparsity Attention postReshape chunk - 3 output: 0.9703776041666666
[Time] - Transpose and LIF: 363067.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6393229166666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96710205078125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
[Time] - Spikeformer Time time: 0.0488 seconds
Run 13 - Tempo totale del modello: 0.0488 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6918131510416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9696451822916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9691569010416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96923828125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9696451822916666
sparsity Attention k_chunks: 0.9691569010416666
sparsity Attention v_chunks: 0.96923828125
[Time] - Reshape: 263957.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9696451822916666
sparsity Attention postReshape k_chunks: 0.9691569010416666
sparsity Attention postReshape v_chunks: 0.96923828125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 252588.0000 nanoseconds
sparsity Attention out: 0.97705078125
[Time] - Reshape Time-Space: 26597.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9921875
sparsity Attention postReshape chunk - 1 output: 0.9915364583333334
sparsity Attention postReshape chunk - 2 output: 0.9596354166666666
sparsity Attention postReshape chunk - 3 output: 0.96484375
[Time] - Transpose and LIF: 379389.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.68994140625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9693196614583334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.681884765625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9691569010416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9712727864583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698079427083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9691569010416666
sparsity Attention k_chunks: 0.9712727864583334
sparsity Attention v_chunks: 0.9698079427083334
[Time] - Reshape: 251393.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9691569010416666
sparsity Attention postReshape k_chunks: 0.9712727864583334
sparsity Attention postReshape v_chunks: 0.9698079427083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 105601.0000 nanoseconds
sparsity Attention out: 0.9800618489583334
[Time] - Reshape Time-Space: 19659.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9905598958333334
sparsity Attention postReshape chunk - 1 output: 0.9886067708333334
sparsity Attention postReshape chunk - 2 output: 0.9801432291666666
sparsity Attention postReshape chunk - 3 output: 0.9609375
[Time] - Transpose and LIF: 348056.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9957682291666666
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6837565104166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9689534505208334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6774088541666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9718424479166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9696451822916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9718424479166666
sparsity Attention k_chunks: 0.9696451822916666
sparsity Attention v_chunks: 0.969970703125
[Time] - Reshape: 250738.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9718424479166666
sparsity Attention postReshape k_chunks: 0.9696451822916666
sparsity Attention postReshape v_chunks: 0.969970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 269388.0000 nanoseconds
sparsity Attention out: 0.9803873697916666
[Time] - Reshape Time-Space: 25080.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9912109375
sparsity Attention postReshape chunk - 1 output: 0.98828125
sparsity Attention postReshape chunk - 2 output: 0.970703125
sparsity Attention postReshape chunk - 3 output: 0.9713541666666666
[Time] - Transpose and LIF: 349020.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.66796875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9695027669270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6614583333333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9694010416666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9689127604166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9673665364583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9694010416666666
sparsity Attention k_chunks: 0.9689127604166666
sparsity Attention v_chunks: 0.9673665364583334
[Time] - Reshape: 267227.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9694010416666666
sparsity Attention postReshape k_chunks: 0.9689127604166666
sparsity Attention postReshape v_chunks: 0.9673665364583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 250633.0000 nanoseconds
sparsity Attention out: 0.9788411458333334
[Time] - Reshape Time-Space: 27858.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9820963541666666
sparsity Attention postReshape chunk - 1 output: 0.9853515625
sparsity Attention postReshape chunk - 2 output: 0.9713541666666666
sparsity Attention postReshape chunk - 3 output: 0.9765625
[Time] - Transpose and LIF: 368801.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9947916666666666
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6630045572916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679158528645834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.65966796875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9697265625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9694010416666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9697265625
sparsity Attention k_chunks: 0.9694010416666666
sparsity Attention v_chunks: 0.970703125
[Time] - Reshape: 262902.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9697265625
sparsity Attention postReshape k_chunks: 0.9694010416666666
sparsity Attention postReshape v_chunks: 0.970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 104898.0000 nanoseconds
sparsity Attention out: 0.9751790364583334
[Time] - Reshape Time-Space: 19180.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9892578125
sparsity Attention postReshape chunk - 1 output: 0.9794921875
sparsity Attention postReshape chunk - 2 output: 0.958984375
sparsity Attention postReshape chunk - 3 output: 0.9729817708333334
[Time] - Transpose and LIF: 361696.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0038 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6607259114583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9685872395833334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.65771484375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9669596354166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96630859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967041015625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9669596354166666
sparsity Attention k_chunks: 0.96630859375
sparsity Attention v_chunks: 0.967041015625
[Time] - Reshape: 250961.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9669596354166666
sparsity Attention postReshape k_chunks: 0.96630859375
sparsity Attention postReshape v_chunks: 0.967041015625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 268842.0000 nanoseconds
sparsity Attention out: 0.970458984375
[Time] - Reshape Time-Space: 26329.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9794921875
sparsity Attention postReshape chunk - 1 output: 0.9752604166666666
sparsity Attention postReshape chunk - 2 output: 0.9609375
sparsity Attention postReshape chunk - 3 output: 0.9661458333333334
[Time] - Transpose and LIF: 368955.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6583658854166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9678955078125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0016 seconds
sparsity x_for_qkv: 0.6527506510416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9702962239583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9673665364583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.97021484375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9702962239583334
sparsity Attention k_chunks: 0.9673665364583334
sparsity Attention v_chunks: 0.97021484375
[Time] - Reshape: 261628.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9702962239583334
sparsity Attention postReshape k_chunks: 0.9673665364583334
sparsity Attention postReshape v_chunks: 0.97021484375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 251349.0000 nanoseconds
sparsity Attention out: 0.978515625
[Time] - Reshape Time-Space: 26101.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.98046875
sparsity Attention postReshape chunk - 1 output: 0.9827473958333334
sparsity Attention postReshape chunk - 2 output: 0.9697265625
sparsity Attention postReshape chunk - 3 output: 0.9811197916666666
[Time] - Transpose and LIF: 362384.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9970703125
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6529947916666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9675089518229166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0017 seconds
sparsity x_for_qkv: 0.6493326822916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9666341145833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9679361979166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9666341145833334
sparsity Attention k_chunks: 0.9679361979166666
sparsity Attention v_chunks: 0.96875
[Time] - Reshape: 269323.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9666341145833334
sparsity Attention postReshape k_chunks: 0.9679361979166666
sparsity Attention postReshape v_chunks: 0.96875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107690.0000 nanoseconds
sparsity Attention out: 0.9766438802083334
[Time] - Reshape Time-Space: 19436.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9807942708333334
sparsity Attention postReshape chunk - 1 output: 0.9833984375
sparsity Attention postReshape chunk - 2 output: 0.9677734375
sparsity Attention postReshape chunk - 3 output: 0.974609375
[Time] - Transpose and LIF: 432434.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6470540364583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9674275716145834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
[Time] - Spikeformer Time time: 0.0489 seconds
Run 14 - Tempo totale del modello: 0.0489 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6944986979166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969482421875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968505859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96630859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969482421875
sparsity Attention k_chunks: 0.968505859375
sparsity Attention v_chunks: 0.96630859375
[Time] - Reshape: 379551.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969482421875
sparsity Attention postReshape k_chunks: 0.968505859375
sparsity Attention postReshape v_chunks: 0.96630859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107992.0000 nanoseconds
sparsity Attention out: 0.9727376302083334
[Time] - Reshape Time-Space: 20329.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9892578125
sparsity Attention postReshape chunk - 1 output: 0.9879557291666666
sparsity Attention postReshape chunk - 2 output: 0.9446614583333334
sparsity Attention postReshape chunk - 3 output: 0.9690755208333334
[Time] - Transpose and LIF: 349482.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9915364583333334
sparsity Attention postReshape chunk - 3 x: 0.994140625
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6957194010416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9701334635416666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6834309895833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9705403645833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9673665364583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9705403645833334
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.9673665364583334
[Time] - Reshape: 310439.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9705403645833334
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.9673665364583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 263094.0000 nanoseconds
sparsity Attention out: 0.9742024739583334
[Time] - Reshape Time-Space: 25801.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9905598958333334
sparsity Attention postReshape chunk - 1 output: 0.9781901041666666
sparsity Attention postReshape chunk - 2 output: 0.9680989583333334
sparsity Attention postReshape chunk - 3 output: 0.9599609375
[Time] - Transpose and LIF: 369105.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.99609375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6774088541666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679972330729166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6689453125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966796875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9696451822916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9658203125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966796875
sparsity Attention k_chunks: 0.9696451822916666
sparsity Attention v_chunks: 0.9658203125
[Time] - Reshape: 261479.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.966796875
sparsity Attention postReshape k_chunks: 0.9696451822916666
sparsity Attention postReshape v_chunks: 0.9658203125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 255948.0000 nanoseconds
sparsity Attention out: 0.974365234375
[Time] - Reshape Time-Space: 26991.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9778645833333334
sparsity Attention postReshape chunk - 1 output: 0.9814453125
sparsity Attention postReshape chunk - 2 output: 0.9749348958333334
sparsity Attention postReshape chunk - 3 output: 0.9632161458333334
[Time] - Transpose and LIF: 370529.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6717122395833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9682820638020834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.66162109375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9676920572916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676920572916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9672037760416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9676920572916666
sparsity Attention k_chunks: 0.9676920572916666
sparsity Attention v_chunks: 0.9672037760416666
[Time] - Reshape: 255569.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9676920572916666
sparsity Attention postReshape k_chunks: 0.9676920572916666
sparsity Attention postReshape v_chunks: 0.9672037760416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107344.0000 nanoseconds
sparsity Attention out: 0.9762369791666666
[Time] - Reshape Time-Space: 23255.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9720052083333334
sparsity Attention postReshape chunk - 1 output: 0.9811197916666666
sparsity Attention postReshape chunk - 2 output: 0.9759114583333334
sparsity Attention postReshape chunk - 3 output: 0.9759114583333334
[Time] - Transpose and LIF: 362858.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9973958333333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6593424479166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9684855143229166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6558430989583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967529296875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96923828125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967041015625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967529296875
sparsity Attention k_chunks: 0.96923828125
sparsity Attention v_chunks: 0.967041015625
[Time] - Reshape: 256222.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.967529296875
sparsity Attention postReshape k_chunks: 0.96923828125
sparsity Attention postReshape v_chunks: 0.967041015625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 266218.0000 nanoseconds
sparsity Attention out: 0.9754231770833334
[Time] - Reshape Time-Space: 26972.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.98046875
sparsity Attention postReshape chunk - 1 output: 0.9671223958333334
sparsity Attention postReshape chunk - 2 output: 0.974609375
sparsity Attention postReshape chunk - 3 output: 0.9794921875
[Time] - Transpose and LIF: 362892.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6573079427083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676106770833334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6538899739583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9669596354166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966796875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.965576171875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9669596354166666
sparsity Attention k_chunks: 0.966796875
sparsity Attention v_chunks: 0.965576171875
[Time] - Reshape: 255037.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9669596354166666
sparsity Attention postReshape k_chunks: 0.966796875
sparsity Attention postReshape v_chunks: 0.965576171875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 268448.0000 nanoseconds
sparsity Attention out: 0.9734700520833334
[Time] - Reshape Time-Space: 27303.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9827473958333334
sparsity Attention postReshape chunk - 1 output: 0.9632161458333334
sparsity Attention postReshape chunk - 2 output: 0.9752604166666666
sparsity Attention postReshape chunk - 3 output: 0.97265625
[Time] - Transpose and LIF: 369046.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9964192708333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.65185546875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.966796875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6464029947916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96923828125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9686686197916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9671223958333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96923828125
sparsity Attention k_chunks: 0.9686686197916666
sparsity Attention v_chunks: 0.9671223958333334
[Time] - Reshape: 253782.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96923828125
sparsity Attention postReshape k_chunks: 0.9686686197916666
sparsity Attention postReshape v_chunks: 0.9671223958333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107397.0000 nanoseconds
sparsity Attention out: 0.974853515625
[Time] - Reshape Time-Space: 20435.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9856770833333334
sparsity Attention postReshape chunk - 1 output: 0.9788411458333334
sparsity Attention postReshape chunk - 2 output: 0.9654947916666666
sparsity Attention postReshape chunk - 3 output: 0.9694010416666666
[Time] - Transpose and LIF: 362213.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6487630208333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9671630859375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6480305989583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9673665364583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9663899739583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9659830729166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9673665364583334
sparsity Attention k_chunks: 0.9663899739583334
sparsity Attention v_chunks: 0.9659830729166666
[Time] - Reshape: 287579.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9673665364583334
sparsity Attention postReshape k_chunks: 0.9663899739583334
sparsity Attention postReshape v_chunks: 0.9659830729166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 105346.0000 nanoseconds
sparsity Attention out: 0.9707845052083334
[Time] - Reshape Time-Space: 20196.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.974609375
sparsity Attention postReshape chunk - 1 output: 0.9710286458333334
sparsity Attention postReshape chunk - 2 output: 0.9664713541666666
sparsity Attention postReshape chunk - 3 output: 0.9710286458333334
[Time] - Transpose and LIF: 371393.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9954427083333334
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.64599609375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9671427408854166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
[Time] - Spikeformer Time time: 0.0523 seconds
Run 15 - Tempo totale del modello: 0.0523 secondi
[Time] - SPS.PSM: 0.0023 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6815592447916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9720052083333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9713541666666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9722493489583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9720052083333334
sparsity Attention k_chunks: 0.9713541666666666
sparsity Attention v_chunks: 0.9722493489583334
[Time] - Reshape: 253839.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9720052083333334
sparsity Attention postReshape k_chunks: 0.9713541666666666
sparsity Attention postReshape v_chunks: 0.9722493489583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 262192.0000 nanoseconds
sparsity Attention out: 0.984130859375
[Time] - Reshape Time-Space: 27032.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9931640625
sparsity Attention postReshape chunk - 1 output: 0.9892578125
sparsity Attention postReshape chunk - 2 output: 0.9762369791666666
sparsity Attention postReshape chunk - 3 output: 0.9778645833333334
[Time] - Transpose and LIF: 378036.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.68115234375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9710693359375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6725260416666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96923828125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9693196614583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96923828125
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.9693196614583334
[Time] - Reshape: 257550.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96923828125
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.9693196614583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 271358.0000 nanoseconds
sparsity Attention out: 0.9776204427083334
[Time] - Reshape Time-Space: 28050.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9830729166666666
sparsity Attention postReshape chunk - 1 output: 0.9775390625
sparsity Attention postReshape chunk - 2 output: 0.9710286458333334
sparsity Attention postReshape chunk - 3 output: 0.9788411458333334
[Time] - Transpose and LIF: 375792.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9951171875
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6700846354166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9704793294270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6639811197916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9677734375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.969482421875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9695638020833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9677734375
sparsity Attention k_chunks: 0.969482421875
sparsity Attention v_chunks: 0.9695638020833334
[Time] - Reshape: 255725.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9677734375
sparsity Attention postReshape k_chunks: 0.969482421875
sparsity Attention postReshape v_chunks: 0.9695638020833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 104437.0000 nanoseconds
sparsity Attention out: 0.976806640625
[Time] - Reshape Time-Space: 28557.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.990234375
sparsity Attention postReshape chunk - 1 output: 0.9772135416666666
sparsity Attention postReshape chunk - 2 output: 0.978515625
sparsity Attention postReshape chunk - 3 output: 0.9612630208333334
[Time] - Transpose and LIF: 357534.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6656087239583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9681193033854166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.6610514322916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9664713541666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9690755208333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968505859375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9664713541666666
sparsity Attention k_chunks: 0.9690755208333334
sparsity Attention v_chunks: 0.968505859375
[Time] - Reshape: 254765.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9664713541666666
sparsity Attention postReshape k_chunks: 0.9690755208333334
sparsity Attention postReshape v_chunks: 0.968505859375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 272272.0000 nanoseconds
sparsity Attention out: 0.9756673177083334
[Time] - Reshape Time-Space: 26209.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9791666666666666
sparsity Attention postReshape chunk - 1 output: 0.9733072916666666
sparsity Attention postReshape chunk - 2 output: 0.9684244791666666
sparsity Attention postReshape chunk - 3 output: 0.9817708333333334
[Time] - Transpose and LIF: 371153.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.658447265625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96734619140625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6542154947916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9688313802083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968017578125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9700520833333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9688313802083334
sparsity Attention k_chunks: 0.968017578125
sparsity Attention v_chunks: 0.9700520833333334
[Time] - Reshape: 267062.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9688313802083334
sparsity Attention postReshape k_chunks: 0.968017578125
sparsity Attention postReshape v_chunks: 0.9700520833333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 256662.0000 nanoseconds
sparsity Attention out: 0.9825846354166666
[Time] - Reshape Time-Space: 28481.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9876302083333334
sparsity Attention postReshape chunk - 1 output: 0.9915364583333334
sparsity Attention postReshape chunk - 2 output: 0.9713541666666666
sparsity Attention postReshape chunk - 3 output: 0.9798177083333334
[Time] - Transpose and LIF: 369203.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.64892578125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96881103515625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6459147135416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9667154947916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970703125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966064453125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9667154947916666
sparsity Attention k_chunks: 0.970703125
sparsity Attention v_chunks: 0.966064453125
[Time] - Reshape: 364473.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9667154947916666
sparsity Attention postReshape k_chunks: 0.970703125
sparsity Attention postReshape v_chunks: 0.966064453125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 108908.0000 nanoseconds
sparsity Attention out: 0.9771321614583334
[Time] - Reshape Time-Space: 20088.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9752604166666666
sparsity Attention postReshape chunk - 1 output: 0.986328125
sparsity Attention postReshape chunk - 2 output: 0.970703125
sparsity Attention postReshape chunk - 3 output: 0.9762369791666666
[Time] - Transpose and LIF: 376120.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6500651041666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9681396484375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6468098958333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969970703125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9674479166666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9678548177083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969970703125
sparsity Attention k_chunks: 0.9674479166666666
sparsity Attention v_chunks: 0.9678548177083334
[Time] - Reshape: 285019.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969970703125
sparsity Attention postReshape k_chunks: 0.9674479166666666
sparsity Attention postReshape v_chunks: 0.9678548177083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 105960.0000 nanoseconds
sparsity Attention out: 0.9724934895833334
[Time] - Reshape Time-Space: 20780.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9778645833333334
sparsity Attention postReshape chunk - 1 output: 0.966796875
sparsity Attention postReshape chunk - 2 output: 0.9703776041666666
sparsity Attention postReshape chunk - 3 output: 0.9749348958333334
[Time] - Transpose and LIF: 347373.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.647705078125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9669596354166666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6451009114583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96728515625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9669596354166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9666341145833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96728515625
sparsity Attention k_chunks: 0.9669596354166666
sparsity Attention v_chunks: 0.9666341145833334
[Time] - Reshape: 256538.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96728515625
sparsity Attention postReshape k_chunks: 0.9669596354166666
sparsity Attention postReshape v_chunks: 0.9666341145833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 259287.0000 nanoseconds
sparsity Attention out: 0.974365234375
[Time] - Reshape Time-Space: 27363.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9817708333333334
sparsity Attention postReshape chunk - 1 output: 0.970703125
sparsity Attention postReshape chunk - 2 output: 0.9729817708333334
sparsity Attention postReshape chunk - 3 output: 0.9720052083333334
[Time] - Transpose and LIF: 354106.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6438802083333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9665934244791666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
[Time] - Spikeformer Time time: 0.0530 seconds
Run 16 - Tempo totale del modello: 0.0531 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.689208984375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9695638020833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676920572916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9695638020833334
sparsity Attention k_chunks: 0.96875
sparsity Attention v_chunks: 0.9676920572916666
[Time] - Reshape: 263115.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9695638020833334
sparsity Attention postReshape k_chunks: 0.96875
sparsity Attention postReshape v_chunks: 0.9676920572916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 256925.0000 nanoseconds
sparsity Attention out: 0.9735514322916666
[Time] - Reshape Time-Space: 27037.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9905598958333334
sparsity Attention postReshape chunk - 1 output: 0.9840494791666666
sparsity Attention postReshape chunk - 2 output: 0.9622395833333334
sparsity Attention postReshape chunk - 3 output: 0.9573567708333334
[Time] - Transpose and LIF: 381363.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.68359375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9684651692708334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6695149739583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969482421875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9689127604166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698079427083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969482421875
sparsity Attention k_chunks: 0.9689127604166666
sparsity Attention v_chunks: 0.9698079427083334
[Time] - Reshape: 310520.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969482421875
sparsity Attention postReshape k_chunks: 0.9689127604166666
sparsity Attention postReshape v_chunks: 0.9698079427083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 112369.0000 nanoseconds
sparsity Attention out: 0.9752604166666666
[Time] - Reshape Time-Space: 21093.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9889322916666666
sparsity Attention postReshape chunk - 1 output: 0.9811197916666666
sparsity Attention postReshape chunk - 2 output: 0.9638671875
sparsity Attention postReshape chunk - 3 output: 0.9671223958333334
[Time] - Transpose and LIF: 365478.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6682942708333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96905517578125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.665771484375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9696451822916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.97021484375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968017578125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9696451822916666
sparsity Attention k_chunks: 0.97021484375
sparsity Attention v_chunks: 0.968017578125
[Time] - Reshape: 253935.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9696451822916666
sparsity Attention postReshape k_chunks: 0.97021484375
sparsity Attention postReshape v_chunks: 0.968017578125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 271129.0000 nanoseconds
sparsity Attention out: 0.9749348958333334
[Time] - Reshape Time-Space: 27516.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9814453125
sparsity Attention postReshape chunk - 1 output: 0.9742838541666666
sparsity Attention postReshape chunk - 2 output: 0.974609375
sparsity Attention postReshape chunk - 3 output: 0.9694010416666666
[Time] - Transpose and LIF: 367561.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6660970052083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9701944986979166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6620279947916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.970458984375
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9696451822916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9666341145833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.970458984375
sparsity Attention k_chunks: 0.9696451822916666
sparsity Attention v_chunks: 0.9666341145833334
[Time] - Reshape: 252413.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.970458984375
sparsity Attention postReshape k_chunks: 0.9696451822916666
sparsity Attention postReshape v_chunks: 0.9666341145833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 254746.0000 nanoseconds
sparsity Attention out: 0.9764811197916666
[Time] - Reshape Time-Space: 26975.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9918619791666666
sparsity Attention postReshape chunk - 1 output: 0.9840494791666666
sparsity Attention postReshape chunk - 2 output: 0.9742838541666666
sparsity Attention postReshape chunk - 3 output: 0.9557291666666666
[Time] - Transpose and LIF: 456455.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6598307291666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96966552734375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.6553548177083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967529296875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9686686197916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96826171875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967529296875
sparsity Attention k_chunks: 0.9686686197916666
sparsity Attention v_chunks: 0.96826171875
[Time] - Reshape: 308027.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.967529296875
sparsity Attention postReshape k_chunks: 0.9686686197916666
sparsity Attention postReshape v_chunks: 0.96826171875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 105429.0000 nanoseconds
sparsity Attention out: 0.975830078125
[Time] - Reshape Time-Space: 19810.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9954427083333334
sparsity Attention postReshape chunk - 1 output: 0.9759114583333334
sparsity Attention postReshape chunk - 2 output: 0.9625651041666666
sparsity Attention postReshape chunk - 3 output: 0.9694010416666666
[Time] - Transpose and LIF: 353344.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6575520833333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676513671875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6585286458333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9657389322916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9662272135416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96728515625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9657389322916666
sparsity Attention k_chunks: 0.9662272135416666
sparsity Attention v_chunks: 0.96728515625
[Time] - Reshape: 253148.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9657389322916666
sparsity Attention postReshape k_chunks: 0.9662272135416666
sparsity Attention postReshape v_chunks: 0.96728515625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 106099.0000 nanoseconds
sparsity Attention out: 0.9718424479166666
[Time] - Reshape Time-Space: 20184.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9772135416666666
sparsity Attention postReshape chunk - 1 output: 0.9755859375
sparsity Attention postReshape chunk - 2 output: 0.9544270833333334
sparsity Attention postReshape chunk - 3 output: 0.9801432291666666
[Time] - Transpose and LIF: 361084.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9967447916666666
sparsity Attention postReshape chunk - 2 x: 0.9957682291666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6569010416666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9681396484375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6548665364583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9694010416666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9688313802083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9694010416666666
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.9688313802083334
[Time] - Reshape: 252234.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9694010416666666
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.9688313802083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 267288.0000 nanoseconds
sparsity Attention out: 0.9783528645833334
[Time] - Reshape Time-Space: 28707.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9850260416666666
sparsity Attention postReshape chunk - 1 output: 0.9827473958333334
sparsity Attention postReshape chunk - 2 output: 0.9739583333333334
sparsity Attention postReshape chunk - 3 output: 0.9716796875
[Time] - Transpose and LIF: 364611.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6549479166666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677327473958334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6524251302083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9680989583333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9672037760416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9680989583333334
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.9672037760416666
[Time] - Reshape: 259353.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9680989583333334
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.9672037760416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 253813.0000 nanoseconds
sparsity Attention out: 0.974609375
[Time] - Reshape Time-Space: 26023.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9794921875
sparsity Attention postReshape chunk - 1 output: 0.9755859375
sparsity Attention postReshape chunk - 2 output: 0.9694010416666666
sparsity Attention postReshape chunk - 3 output: 0.9739583333333334
[Time] - Transpose and LIF: 347342.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0046 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.651611328125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9674072265625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
[Time] - Spikeformer Time time: 0.0531 seconds
Run 17 - Tempo totale del modello: 0.0531 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.7000325520833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9685872395833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9702962239583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96875
sparsity Attention k_chunks: 0.9685872395833334
sparsity Attention v_chunks: 0.9702962239583334
[Time] - Reshape: 291178.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96875
sparsity Attention postReshape k_chunks: 0.9685872395833334
sparsity Attention postReshape v_chunks: 0.9702962239583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107225.0000 nanoseconds
sparsity Attention out: 0.9729817708333334
[Time] - Reshape Time-Space: 20983.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9983723958333334
sparsity Attention postReshape chunk - 1 output: 0.9889322916666666
sparsity Attention postReshape chunk - 2 output: 0.9609375
sparsity Attention postReshape chunk - 3 output: 0.9436848958333334
[Time] - Transpose and LIF: 351789.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.694580078125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9710693359375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.685302734375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9706217447916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968994140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9681803385416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9706217447916666
sparsity Attention k_chunks: 0.968994140625
sparsity Attention v_chunks: 0.9681803385416666
[Time] - Reshape: 260027.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9706217447916666
sparsity Attention postReshape k_chunks: 0.968994140625
sparsity Attention postReshape v_chunks: 0.9681803385416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 260694.0000 nanoseconds
sparsity Attention out: 0.9715983072916666
[Time] - Reshape Time-Space: 36397.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9905598958333334
sparsity Attention postReshape chunk - 1 output: 0.9908854166666666
sparsity Attention postReshape chunk - 2 output: 0.9534505208333334
sparsity Attention postReshape chunk - 3 output: 0.9514973958333334
[Time] - Transpose and LIF: 367603.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9954427083333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.686767578125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9695027669270834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6790364583333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9700520833333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9717610677083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9700520833333334
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.9717610677083334
[Time] - Reshape: 255322.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9700520833333334
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.9717610677083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 261884.0000 nanoseconds
sparsity Attention out: 0.979248046875
[Time] - Reshape Time-Space: 27760.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.990234375
sparsity Attention postReshape chunk - 1 output: 0.98828125
sparsity Attention postReshape chunk - 2 output: 0.97265625
sparsity Attention postReshape chunk - 3 output: 0.9658203125
[Time] - Transpose and LIF: 376863.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9973958333333334
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6767578125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9682210286458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6712239583333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9698079427083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9671223958333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966796875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9698079427083334
sparsity Attention k_chunks: 0.9671223958333334
sparsity Attention v_chunks: 0.966796875
[Time] - Reshape: 256696.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9698079427083334
sparsity Attention postReshape k_chunks: 0.9671223958333334
sparsity Attention postReshape v_chunks: 0.966796875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 108834.0000 nanoseconds
sparsity Attention out: 0.9688313802083334
[Time] - Reshape Time-Space: 20492.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9830729166666666
sparsity Attention postReshape chunk - 1 output: 0.9720052083333334
sparsity Attention postReshape chunk - 2 output: 0.9541015625
sparsity Attention postReshape chunk - 3 output: 0.9661458333333334
[Time] - Transpose and LIF: 361884.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.667724609375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96771240234375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6597493489583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968017578125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9684244791666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698079427083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968017578125
sparsity Attention k_chunks: 0.9684244791666666
sparsity Attention v_chunks: 0.9698079427083334
[Time] - Reshape: 307783.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.968017578125
sparsity Attention postReshape k_chunks: 0.9684244791666666
sparsity Attention postReshape v_chunks: 0.9698079427083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 109494.0000 nanoseconds
sparsity Attention out: 0.9800618489583334
[Time] - Reshape Time-Space: 23291.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.990234375
sparsity Attention postReshape chunk - 1 output: 0.9908854166666666
sparsity Attention postReshape chunk - 2 output: 0.9710286458333334
sparsity Attention postReshape chunk - 3 output: 0.9680989583333334
[Time] - Transpose and LIF: 365964.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.66357421875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672444661458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6573079427083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9662272135416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9674479166666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96923828125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9662272135416666
sparsity Attention k_chunks: 0.9674479166666666
sparsity Attention v_chunks: 0.96923828125
[Time] - Reshape: 254429.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9662272135416666
sparsity Attention postReshape k_chunks: 0.9674479166666666
sparsity Attention postReshape v_chunks: 0.96923828125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 269170.0000 nanoseconds
sparsity Attention out: 0.9737955729166666
[Time] - Reshape Time-Space: 29086.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.990234375
sparsity Attention postReshape chunk - 1 output: 0.9788411458333334
sparsity Attention postReshape chunk - 2 output: 0.9680989583333334
sparsity Attention postReshape chunk - 3 output: 0.9580078125
[Time] - Transpose and LIF: 370866.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9967447916666666
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6604817708333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676513671875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6538899739583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9652506510416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9654134114583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96533203125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9652506510416666
sparsity Attention k_chunks: 0.9654134114583334
sparsity Attention v_chunks: 0.96533203125
[Time] - Reshape: 339748.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9652506510416666
sparsity Attention postReshape k_chunks: 0.9654134114583334
sparsity Attention postReshape v_chunks: 0.96533203125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 251744.0000 nanoseconds
sparsity Attention out: 0.9685872395833334
[Time] - Reshape Time-Space: 26504.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.974609375
sparsity Attention postReshape chunk - 1 output: 0.9720052083333334
sparsity Attention postReshape chunk - 2 output: 0.9671223958333334
sparsity Attention postReshape chunk - 3 output: 0.9606119791666666
[Time] - Transpose and LIF: 374174.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.654541015625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9668986002604166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.6499837239583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9669596354166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968017578125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9681803385416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9669596354166666
sparsity Attention k_chunks: 0.968017578125
sparsity Attention v_chunks: 0.9681803385416666
[Time] - Reshape: 307951.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9669596354166666
sparsity Attention postReshape k_chunks: 0.968017578125
sparsity Attention postReshape v_chunks: 0.9681803385416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 106000.0000 nanoseconds
sparsity Attention out: 0.9759114583333334
[Time] - Reshape Time-Space: 20409.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9801432291666666
sparsity Attention postReshape chunk - 1 output: 0.9794921875
sparsity Attention postReshape chunk - 2 output: 0.9759114583333334
sparsity Attention postReshape chunk - 3 output: 0.9680989583333334
[Time] - Transpose and LIF: 364585.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9973958333333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.650390625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9667154947916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
[Time] - Spikeformer Time time: 0.0533 seconds
Run 18 - Tempo totale del modello: 0.0533 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6923014322916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9701334635416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9720052083333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9713541666666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9701334635416666
sparsity Attention k_chunks: 0.9720052083333334
sparsity Attention v_chunks: 0.9713541666666666
[Time] - Reshape: 312148.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9701334635416666
sparsity Attention postReshape k_chunks: 0.9720052083333334
sparsity Attention postReshape v_chunks: 0.9713541666666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 258127.0000 nanoseconds
sparsity Attention out: 0.9781901041666666
[Time] - Reshape Time-Space: 28608.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9967447916666666
sparsity Attention postReshape chunk - 1 output: 0.990234375
sparsity Attention postReshape chunk - 2 output: 0.96875
sparsity Attention postReshape chunk - 3 output: 0.95703125
[Time] - Transpose and LIF: 379591.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.68701171875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9701131184895834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6753743489583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96923828125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9702962239583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96923828125
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.9702962239583334
[Time] - Reshape: 262832.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96923828125
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.9702962239583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 253426.0000 nanoseconds
sparsity Attention out: 0.9813639322916666
[Time] - Reshape Time-Space: 27774.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9947916666666666
sparsity Attention postReshape chunk - 1 output: 0.9899088541666666
sparsity Attention postReshape chunk - 2 output: 0.9654947916666666
sparsity Attention postReshape chunk - 3 output: 0.9752604166666666
[Time] - Transpose and LIF: 368628.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6830240885416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9701131184895834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.6739908854166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9698079427083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9695638020833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96923828125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9698079427083334
sparsity Attention k_chunks: 0.9695638020833334
sparsity Attention v_chunks: 0.96923828125
[Time] - Reshape: 252388.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9698079427083334
sparsity Attention postReshape k_chunks: 0.9695638020833334
sparsity Attention postReshape v_chunks: 0.96923828125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 115900.0000 nanoseconds
sparsity Attention out: 0.980712890625
[Time] - Reshape Time-Space: 19843.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9827473958333334
sparsity Attention postReshape chunk - 1 output: 0.9837239583333334
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.9794921875
[Time] - Transpose and LIF: 360381.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6689453125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9710286458333334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.665771484375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9688313802083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9696451822916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9702962239583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9688313802083334
sparsity Attention k_chunks: 0.9696451822916666
sparsity Attention v_chunks: 0.9702962239583334
[Time] - Reshape: 283337.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9688313802083334
sparsity Attention postReshape k_chunks: 0.9696451822916666
sparsity Attention postReshape v_chunks: 0.9702962239583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107298.0000 nanoseconds
sparsity Attention out: 0.9810384114583334
[Time] - Reshape Time-Space: 20955.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9853515625
sparsity Attention postReshape chunk - 1 output: 0.9788411458333334
sparsity Attention postReshape chunk - 2 output: 0.9817708333333334
sparsity Attention postReshape chunk - 3 output: 0.9781901041666666
[Time] - Transpose and LIF: 341150.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.66357421875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9693196614583334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6603190104166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9691569010416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9668782552083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9677734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9691569010416666
sparsity Attention k_chunks: 0.9668782552083334
sparsity Attention v_chunks: 0.9677734375
[Time] - Reshape: 256136.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9691569010416666
sparsity Attention postReshape k_chunks: 0.9668782552083334
sparsity Attention postReshape v_chunks: 0.9677734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 254553.0000 nanoseconds
sparsity Attention out: 0.9727376302083334
[Time] - Reshape Time-Space: 28257.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9723307291666666
sparsity Attention postReshape chunk - 1 output: 0.9830729166666666
sparsity Attention postReshape chunk - 2 output: 0.9619140625
sparsity Attention postReshape chunk - 3 output: 0.9736328125
[Time] - Transpose and LIF: 368100.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.65673828125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9685465494791666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.652587890625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9688313802083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9690755208333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9674479166666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9688313802083334
sparsity Attention k_chunks: 0.9690755208333334
sparsity Attention v_chunks: 0.9674479166666666
[Time] - Reshape: 257023.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9688313802083334
sparsity Attention postReshape k_chunks: 0.9690755208333334
sparsity Attention postReshape v_chunks: 0.9674479166666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 106777.0000 nanoseconds
sparsity Attention out: 0.9794108072916666
[Time] - Reshape Time-Space: 21108.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9856770833333334
sparsity Attention postReshape chunk - 1 output: 0.9729817708333334
sparsity Attention postReshape chunk - 2 output: 0.9850260416666666
sparsity Attention postReshape chunk - 3 output: 0.9739583333333334
[Time] - Transpose and LIF: 367625.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6499837239583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679361979166666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6451822916666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.970703125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676106770833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968994140625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.970703125
sparsity Attention k_chunks: 0.9676106770833334
sparsity Attention v_chunks: 0.968994140625
[Time] - Reshape: 336863.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.970703125
sparsity Attention postReshape k_chunks: 0.9676106770833334
sparsity Attention postReshape v_chunks: 0.968994140625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 106009.0000 nanoseconds
sparsity Attention out: 0.97802734375
[Time] - Reshape Time-Space: 20480.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9733072916666666
sparsity Attention postReshape chunk - 1 output: 0.9817708333333334
sparsity Attention postReshape chunk - 2 output: 0.9817708333333334
sparsity Attention postReshape chunk - 3 output: 0.9752604166666666
[Time] - Transpose and LIF: 354448.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.64306640625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9688720703125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6424967447916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9674479166666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968505859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9664713541666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9674479166666666
sparsity Attention k_chunks: 0.968505859375
sparsity Attention v_chunks: 0.9664713541666666
[Time] - Reshape: 279736.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9674479166666666
sparsity Attention postReshape k_chunks: 0.968505859375
sparsity Attention postReshape v_chunks: 0.9664713541666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 276314.0000 nanoseconds
sparsity Attention out: 0.9740397135416666
[Time] - Reshape Time-Space: 28061.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9814453125
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.96484375
sparsity Attention postReshape chunk - 3 output: 0.9697265625
[Time] - Transpose and LIF: 365188.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6453450520833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676717122395834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
[Time] - Spikeformer Time time: 0.0527 seconds
Run 19 - Tempo totale del modello: 0.0528 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6873372395833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9690755208333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9706217447916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96923828125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9690755208333334
sparsity Attention k_chunks: 0.9706217447916666
sparsity Attention v_chunks: 0.96923828125
[Time] - Reshape: 260007.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9690755208333334
sparsity Attention postReshape k_chunks: 0.9706217447916666
sparsity Attention postReshape v_chunks: 0.96923828125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 271407.0000 nanoseconds
sparsity Attention out: 0.979248046875
[Time] - Reshape Time-Space: 35622.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9973958333333334
sparsity Attention postReshape chunk - 1 output: 0.9886067708333334
sparsity Attention postReshape chunk - 2 output: 0.9710286458333334
sparsity Attention postReshape chunk - 3 output: 0.9599609375
[Time] - Transpose and LIF: 370368.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.99609375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6824544270833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9713134765625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6759440104166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9720052083333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.970947265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96826171875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9720052083333334
sparsity Attention k_chunks: 0.970947265625
sparsity Attention v_chunks: 0.96826171875
[Time] - Reshape: 254720.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9720052083333334
sparsity Attention postReshape k_chunks: 0.970947265625
sparsity Attention postReshape v_chunks: 0.96826171875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 122174.0000 nanoseconds
sparsity Attention out: 0.9766438802083334
[Time] - Reshape Time-Space: 20507.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.990234375
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.9749348958333334
sparsity Attention postReshape chunk - 3 output: 0.9609375
[Time] - Transpose and LIF: 367470.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6796875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.97003173828125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6666666666666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9697265625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.97021484375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9686686197916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9697265625
sparsity Attention k_chunks: 0.97021484375
sparsity Attention v_chunks: 0.9686686197916666
[Time] - Reshape: 262276.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9697265625
sparsity Attention postReshape k_chunks: 0.97021484375
sparsity Attention postReshape v_chunks: 0.9686686197916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 263578.0000 nanoseconds
sparsity Attention out: 0.97998046875
[Time] - Reshape Time-Space: 27465.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9905598958333334
sparsity Attention postReshape chunk - 1 output: 0.982421875
sparsity Attention postReshape chunk - 2 output: 0.9759114583333334
sparsity Attention postReshape chunk - 3 output: 0.9710286458333334
[Time] - Transpose and LIF: 817929.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0046 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.664306640625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690348307291666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6593424479166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96826171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9685872395833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9702962239583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96826171875
sparsity Attention k_chunks: 0.9685872395833334
sparsity Attention v_chunks: 0.9702962239583334
[Time] - Reshape: 261020.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96826171875
sparsity Attention postReshape k_chunks: 0.9685872395833334
sparsity Attention postReshape v_chunks: 0.9702962239583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 272955.0000 nanoseconds
sparsity Attention out: 0.974853515625
[Time] - Reshape Time-Space: 29585.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9807942708333334
sparsity Attention postReshape chunk - 1 output: 0.9739583333333334
sparsity Attention postReshape chunk - 2 output: 0.9752604166666666
sparsity Attention postReshape chunk - 3 output: 0.9694010416666666
[Time] - Transpose and LIF: 373216.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6558430989583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9678955078125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6543782552083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9690755208333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9689127604166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9667154947916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9690755208333334
sparsity Attention k_chunks: 0.9689127604166666
sparsity Attention v_chunks: 0.9667154947916666
[Time] - Reshape: 307091.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9690755208333334
sparsity Attention postReshape k_chunks: 0.9689127604166666
sparsity Attention postReshape v_chunks: 0.9667154947916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 109340.0000 nanoseconds
sparsity Attention out: 0.9756673177083334
[Time] - Reshape Time-Space: 20808.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9833984375
sparsity Attention postReshape chunk - 1 output: 0.9762369791666666
sparsity Attention postReshape chunk - 2 output: 0.966796875
sparsity Attention postReshape chunk - 3 output: 0.9762369791666666
[Time] - Transpose and LIF: 366109.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9973958333333334
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6533203125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9687906901041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.6494140625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967041015625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.965576171875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9681803385416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967041015625
sparsity Attention k_chunks: 0.965576171875
sparsity Attention v_chunks: 0.9681803385416666
[Time] - Reshape: 260464.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.967041015625
sparsity Attention postReshape k_chunks: 0.965576171875
sparsity Attention postReshape v_chunks: 0.9681803385416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 105655.0000 nanoseconds
sparsity Attention out: 0.9768880208333334
[Time] - Reshape Time-Space: 21566.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9742838541666666
sparsity Attention postReshape chunk - 1 output: 0.9752604166666666
sparsity Attention postReshape chunk - 2 output: 0.9801432291666666
sparsity Attention postReshape chunk - 3 output: 0.9778645833333334
[Time] - Transpose and LIF: 370395.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6525065104166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96844482421875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.6453450520833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.966796875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9688313802083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9662272135416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.966796875
sparsity Attention k_chunks: 0.9688313802083334
sparsity Attention v_chunks: 0.9662272135416666
[Time] - Reshape: 256540.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.966796875
sparsity Attention postReshape k_chunks: 0.9688313802083334
sparsity Attention postReshape v_chunks: 0.9662272135416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 878736.0000 nanoseconds
sparsity Attention out: 0.9757486979166666
[Time] - Reshape Time-Space: 29293.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9768880208333334
sparsity Attention postReshape chunk - 1 output: 0.9778645833333334
sparsity Attention postReshape chunk - 2 output: 0.9807942708333334
sparsity Attention postReshape chunk - 3 output: 0.9674479166666666
[Time] - Transpose and LIF: 365180.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9967447916666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0047 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6449381510416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9671427408854166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.642578125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9659830729166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9689127604166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968017578125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9659830729166666
sparsity Attention k_chunks: 0.9689127604166666
sparsity Attention v_chunks: 0.968017578125
[Time] - Reshape: 275585.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9659830729166666
sparsity Attention postReshape k_chunks: 0.9689127604166666
sparsity Attention postReshape v_chunks: 0.968017578125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 279342.0000 nanoseconds
sparsity Attention out: 0.9740397135416666
[Time] - Reshape Time-Space: 29713.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9710286458333334
sparsity Attention postReshape chunk - 1 output: 0.9781901041666666
sparsity Attention postReshape chunk - 2 output: 0.9759114583333334
sparsity Attention postReshape chunk - 3 output: 0.9710286458333334
[Time] - Transpose and LIF: 473869.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9970703125
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0045 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.642578125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9666951497395834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
[Time] - Spikeformer Time time: 0.0545 seconds
Run 20 - Tempo totale del modello: 0.0545 secondi
[Time] - SPS.PSM: 0.0020 seconds
[Time] - SPS.RPE: 0.0005 seconds
sparsity x_for_qkv: 0.6837565104166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.97119140625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.974609375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9720052083333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.97119140625
sparsity Attention k_chunks: 0.974609375
sparsity Attention v_chunks: 0.9720052083333334
[Time] - Reshape: 304183.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.97119140625
sparsity Attention postReshape k_chunks: 0.974609375
sparsity Attention postReshape v_chunks: 0.9720052083333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 126617.0000 nanoseconds
sparsity Attention out: 0.9806315104166666
[Time] - Reshape Time-Space: 25807.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9944661458333334
sparsity Attention postReshape chunk - 1 output: 0.9830729166666666
sparsity Attention postReshape chunk - 2 output: 0.9762369791666666
sparsity Attention postReshape chunk - 3 output: 0.96875
[Time] - Transpose and LIF: 420895.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0048 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6875813802083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9706013997395834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0025 seconds
sparsity x_for_qkv: 0.6793619791666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9715169270833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9706217447916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96875
sparsity Attention k_chunks: 0.9715169270833334
sparsity Attention v_chunks: 0.9706217447916666
[Time] - Reshape: 290973.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96875
sparsity Attention postReshape k_chunks: 0.9715169270833334
sparsity Attention postReshape v_chunks: 0.9706217447916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 123604.0000 nanoseconds
sparsity Attention out: 0.9776204427083334
[Time] - Reshape Time-Space: 26144.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.994140625
sparsity Attention postReshape chunk - 1 output: 0.9768880208333334
sparsity Attention postReshape chunk - 2 output: 0.9654947916666666
sparsity Attention postReshape chunk - 3 output: 0.9739583333333334
[Time] - Transpose and LIF: 406771.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9964192708333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0045 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6812337239583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9696451822916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0024 seconds
sparsity x_for_qkv: 0.6708170572916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96826171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9694010416666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698079427083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96826171875
sparsity Attention k_chunks: 0.9694010416666666
sparsity Attention v_chunks: 0.9698079427083334
[Time] - Reshape: 291136.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96826171875
sparsity Attention postReshape k_chunks: 0.9694010416666666
sparsity Attention postReshape v_chunks: 0.9698079427083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 274302.0000 nanoseconds
sparsity Attention out: 0.97509765625
[Time] - Reshape Time-Space: 28831.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9749348958333334
sparsity Attention postReshape chunk - 1 output: 0.984375
sparsity Attention postReshape chunk - 2 output: 0.9729817708333334
sparsity Attention postReshape chunk - 3 output: 0.9680989583333334
[Time] - Transpose and LIF: 372834.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9973958333333334
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0043 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6703287760416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9696451822916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.660400390625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9666341145833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9686686197916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967529296875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9666341145833334
sparsity Attention k_chunks: 0.9686686197916666
sparsity Attention v_chunks: 0.967529296875
[Time] - Reshape: 265023.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9666341145833334
sparsity Attention postReshape k_chunks: 0.9686686197916666
sparsity Attention postReshape v_chunks: 0.967529296875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 274490.0000 nanoseconds
sparsity Attention out: 0.9728190104166666
[Time] - Reshape Time-Space: 29693.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9801432291666666
sparsity Attention postReshape chunk - 1 output: 0.9739583333333334
sparsity Attention postReshape chunk - 2 output: 0.9739583333333334
sparsity Attention postReshape chunk - 3 output: 0.9632161458333334
[Time] - Transpose and LIF: 374837.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0043 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6538899739583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9697672526041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6533203125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9680989583333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9673665364583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9678548177083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9680989583333334
sparsity Attention k_chunks: 0.9673665364583334
sparsity Attention v_chunks: 0.9678548177083334
[Time] - Reshape: 254680.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9680989583333334
sparsity Attention postReshape k_chunks: 0.9673665364583334
sparsity Attention postReshape v_chunks: 0.9678548177083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 106818.0000 nanoseconds
sparsity Attention out: 0.97509765625
[Time] - Reshape Time-Space: 20362.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9847005208333334
sparsity Attention postReshape chunk - 1 output: 0.9739583333333334
sparsity Attention postReshape chunk - 2 output: 0.97265625
sparsity Attention postReshape chunk - 3 output: 0.9690755208333334
[Time] - Transpose and LIF: 342212.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6510416666666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9684651692708334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.64697265625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9673665364583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9681803385416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9691569010416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9673665364583334
sparsity Attention k_chunks: 0.9681803385416666
sparsity Attention v_chunks: 0.9691569010416666
[Time] - Reshape: 250421.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9673665364583334
sparsity Attention postReshape k_chunks: 0.9681803385416666
sparsity Attention postReshape v_chunks: 0.9691569010416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 263605.0000 nanoseconds
sparsity Attention out: 0.9776204427083334
[Time] - Reshape Time-Space: 27458.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9798177083333334
sparsity Attention postReshape chunk - 1 output: 0.978515625
sparsity Attention postReshape chunk - 2 output: 0.9755859375
sparsity Attention postReshape chunk - 3 output: 0.9765625
[Time] - Transpose and LIF: 359125.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9970703125
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9947916666666666
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6437174479166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679972330729166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6456705729166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9684244791666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9669596354166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9674479166666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9684244791666666
sparsity Attention k_chunks: 0.9669596354166666
sparsity Attention v_chunks: 0.9674479166666666
[Time] - Reshape: 277623.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9684244791666666
sparsity Attention postReshape k_chunks: 0.9669596354166666
sparsity Attention postReshape v_chunks: 0.9674479166666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 257521.0000 nanoseconds
sparsity Attention out: 0.9757486979166666
[Time] - Reshape Time-Space: 29450.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9723307291666666
sparsity Attention postReshape chunk - 1 output: 0.9729817708333334
sparsity Attention postReshape chunk - 2 output: 0.9794921875
sparsity Attention postReshape chunk - 3 output: 0.9781901041666666
[Time] - Transpose and LIF: 370409.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9964192708333334
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.998046875
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6443684895833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9665934244791666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6427408854166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967529296875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9658203125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.965576171875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967529296875
sparsity Attention k_chunks: 0.9658203125
sparsity Attention v_chunks: 0.965576171875
[Time] - Reshape: 306694.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.967529296875
sparsity Attention postReshape k_chunks: 0.9658203125
sparsity Attention postReshape v_chunks: 0.965576171875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 105087.0000 nanoseconds
sparsity Attention out: 0.967529296875
[Time] - Reshape Time-Space: 20468.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9661458333333334
sparsity Attention postReshape chunk - 1 output: 0.9625651041666666
sparsity Attention postReshape chunk - 2 output: 0.970703125
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 350434.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.9967447916666666
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6444498697916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96844482421875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
[Time] - Spikeformer Time time: 0.0552 seconds
Run 21 - Tempo totale del modello: 0.0552 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6869303385416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96826171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698079427083334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9702962239583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96826171875
sparsity Attention k_chunks: 0.9698079427083334
sparsity Attention v_chunks: 0.9702962239583334
[Time] - Reshape: 307260.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96826171875
sparsity Attention postReshape k_chunks: 0.9698079427083334
sparsity Attention postReshape v_chunks: 0.9702962239583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 115399.0000 nanoseconds
sparsity Attention out: 0.9737955729166666
[Time] - Reshape Time-Space: 21432.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9938151041666666
sparsity Attention postReshape chunk - 1 output: 0.9772135416666666
sparsity Attention postReshape chunk - 2 output: 0.9557291666666666
sparsity Attention postReshape chunk - 3 output: 0.9684244791666666
[Time] - Transpose and LIF: 360976.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9964192708333334
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6805013020833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.969482421875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.67041015625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9690755208333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969482421875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96875
sparsity Attention k_chunks: 0.9690755208333334
sparsity Attention v_chunks: 0.969482421875
[Time] - Reshape: 304945.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96875
sparsity Attention postReshape k_chunks: 0.9690755208333334
sparsity Attention postReshape v_chunks: 0.969482421875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 259740.0000 nanoseconds
sparsity Attention out: 0.974365234375
[Time] - Reshape Time-Space: 28599.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9921875
sparsity Attention postReshape chunk - 1 output: 0.9820963541666666
sparsity Attention postReshape chunk - 2 output: 0.9524739583333334
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 396251.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6744791666666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9702555338541666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6660970052083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9691569010416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9693196614583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967041015625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9691569010416666
sparsity Attention k_chunks: 0.9693196614583334
sparsity Attention v_chunks: 0.967041015625
[Time] - Reshape: 261562.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9691569010416666
sparsity Attention postReshape k_chunks: 0.9693196614583334
sparsity Attention postReshape v_chunks: 0.967041015625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 254325.0000 nanoseconds
sparsity Attention out: 0.97900390625
[Time] - Reshape Time-Space: 26551.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9850260416666666
sparsity Attention postReshape chunk - 1 output: 0.9817708333333334
sparsity Attention postReshape chunk - 2 output: 0.9736328125
sparsity Attention postReshape chunk - 3 output: 0.9755859375
[Time] - Transpose and LIF: 359726.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9934895833333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6669108072916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9682210286458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.6653645833333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9696451822916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967529296875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9661458333333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9696451822916666
sparsity Attention k_chunks: 0.967529296875
sparsity Attention v_chunks: 0.9661458333333334
[Time] - Reshape: 279771.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9696451822916666
sparsity Attention postReshape k_chunks: 0.967529296875
sparsity Attention postReshape v_chunks: 0.9661458333333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107731.0000 nanoseconds
sparsity Attention out: 0.9732259114583334
[Time] - Reshape Time-Space: 20922.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9830729166666666
sparsity Attention postReshape chunk - 1 output: 0.9820963541666666
sparsity Attention postReshape chunk - 2 output: 0.970703125
sparsity Attention postReshape chunk - 3 output: 0.95703125
[Time] - Transpose and LIF: 378352.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.994140625
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.65966796875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9671834309895834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.6553548177083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9685872395833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967529296875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96923828125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9685872395833334
sparsity Attention k_chunks: 0.967529296875
sparsity Attention v_chunks: 0.96923828125
[Time] - Reshape: 253890.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9685872395833334
sparsity Attention postReshape k_chunks: 0.967529296875
sparsity Attention postReshape v_chunks: 0.96923828125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 267776.0000 nanoseconds
sparsity Attention out: 0.9746907552083334
[Time] - Reshape Time-Space: 27007.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9794921875
sparsity Attention postReshape chunk - 1 output: 0.986328125
sparsity Attention postReshape chunk - 2 output: 0.9768880208333334
sparsity Attention postReshape chunk - 3 output: 0.9560546875
[Time] - Transpose and LIF: 364256.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6539713541666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672648111979166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6473795572916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9667154947916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9685872395833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.966552734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9667154947916666
sparsity Attention k_chunks: 0.9685872395833334
sparsity Attention v_chunks: 0.966552734375
[Time] - Reshape: 295737.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9667154947916666
sparsity Attention postReshape k_chunks: 0.9685872395833334
sparsity Attention postReshape v_chunks: 0.966552734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 292785.0000 nanoseconds
sparsity Attention out: 0.9720052083333334
[Time] - Reshape Time-Space: 49920.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9723307291666666
sparsity Attention postReshape chunk - 1 output: 0.9775390625
sparsity Attention postReshape chunk - 2 output: 0.9759114583333334
sparsity Attention postReshape chunk - 3 output: 0.9622395833333334
[Time] - Transpose and LIF: 371803.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6461588541666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9667561848958334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6439615885416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9654947916666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9677734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9658203125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9654947916666666
sparsity Attention k_chunks: 0.9677734375
sparsity Attention v_chunks: 0.9658203125
[Time] - Reshape: 361520.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9654947916666666
sparsity Attention postReshape k_chunks: 0.9677734375
sparsity Attention postReshape v_chunks: 0.9658203125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 112931.0000 nanoseconds
sparsity Attention out: 0.9742838541666666
[Time] - Reshape Time-Space: 20135.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9820963541666666
sparsity Attention postReshape chunk - 1 output: 0.9713541666666666
sparsity Attention postReshape chunk - 2 output: 0.9716796875
sparsity Attention postReshape chunk - 3 output: 0.9720052083333334
[Time] - Transpose and LIF: 356439.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.994140625
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.99609375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6434733072916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679361979166666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.643798828125
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9678548177083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676920572916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9678548177083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9678548177083334
sparsity Attention k_chunks: 0.9676920572916666
sparsity Attention v_chunks: 0.9678548177083334
[Time] - Reshape: 256655.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9678548177083334
sparsity Attention postReshape k_chunks: 0.9676920572916666
sparsity Attention postReshape v_chunks: 0.9678548177083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 260223.0000 nanoseconds
sparsity Attention out: 0.9755859375
[Time] - Reshape Time-Space: 26477.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9781901041666666
sparsity Attention postReshape chunk - 1 output: 0.9710286458333334
sparsity Attention postReshape chunk - 2 output: 0.9739583333333334
sparsity Attention postReshape chunk - 3 output: 0.9791666666666666
[Time] - Transpose and LIF: 361280.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6409505208333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96551513671875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
[Time] - Spikeformer Time time: 0.0532 seconds
Run 22 - Tempo totale del modello: 0.0533 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6937662760416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.971923828125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9724934895833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.971923828125
sparsity Attention k_chunks: 0.9724934895833334
sparsity Attention v_chunks: 0.970703125
[Time] - Reshape: 268033.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.971923828125
sparsity Attention postReshape k_chunks: 0.9724934895833334
sparsity Attention postReshape v_chunks: 0.970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 261969.0000 nanoseconds
sparsity Attention out: 0.9768880208333334
[Time] - Reshape Time-Space: 29284.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9921875
sparsity Attention postReshape chunk - 1 output: 0.986328125
sparsity Attention postReshape chunk - 2 output: 0.9700520833333334
sparsity Attention postReshape chunk - 3 output: 0.958984375
[Time] - Transpose and LIF: 373312.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.99609375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.689208984375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9698689778645834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6786295572916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9708658854166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9703776041666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9684244791666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9708658854166666
sparsity Attention k_chunks: 0.9703776041666666
sparsity Attention v_chunks: 0.9684244791666666
[Time] - Reshape: 255779.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9708658854166666
sparsity Attention postReshape k_chunks: 0.9703776041666666
sparsity Attention postReshape v_chunks: 0.9684244791666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 112966.0000 nanoseconds
sparsity Attention out: 0.978271484375
[Time] - Reshape Time-Space: 20802.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9876302083333334
sparsity Attention postReshape chunk - 1 output: 0.98828125
sparsity Attention postReshape chunk - 2 output: 0.9749348958333334
sparsity Attention postReshape chunk - 3 output: 0.9622395833333334
[Time] - Transpose and LIF: 437771.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6736653645833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96844482421875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.668212890625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9672037760416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9693196614583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9695638020833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9672037760416666
sparsity Attention k_chunks: 0.9693196614583334
sparsity Attention v_chunks: 0.9695638020833334
[Time] - Reshape: 255990.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9672037760416666
sparsity Attention postReshape k_chunks: 0.9693196614583334
sparsity Attention postReshape v_chunks: 0.9695638020833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107875.0000 nanoseconds
sparsity Attention out: 0.97802734375
[Time] - Reshape Time-Space: 20518.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9886067708333334
sparsity Attention postReshape chunk - 1 output: 0.9775390625
sparsity Attention postReshape chunk - 2 output: 0.9710286458333334
sparsity Attention postReshape chunk - 3 output: 0.9749348958333334
[Time] - Transpose and LIF: 347618.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6629231770833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9669596354166666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6593424479166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9674479166666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9686686197916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9686686197916666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9674479166666666
sparsity Attention k_chunks: 0.9686686197916666
sparsity Attention v_chunks: 0.9686686197916666
[Time] - Reshape: 286276.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9674479166666666
sparsity Attention postReshape k_chunks: 0.9686686197916666
sparsity Attention postReshape v_chunks: 0.9686686197916666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 253117.0000 nanoseconds
sparsity Attention out: 0.97509765625
[Time] - Reshape Time-Space: 26898.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9840494791666666
sparsity Attention postReshape chunk - 1 output: 0.9772135416666666
sparsity Attention postReshape chunk - 2 output: 0.9697265625
sparsity Attention postReshape chunk - 3 output: 0.9694010416666666
[Time] - Transpose and LIF: 372117.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9964192708333334
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6617838541666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9690348307291666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0018 seconds
sparsity x_for_qkv: 0.6599934895833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9668782552083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9674479166666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9673665364583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9668782552083334
sparsity Attention k_chunks: 0.9674479166666666
sparsity Attention v_chunks: 0.9673665364583334
[Time] - Reshape: 263867.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9668782552083334
sparsity Attention postReshape k_chunks: 0.9674479166666666
sparsity Attention postReshape v_chunks: 0.9673665364583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 106992.0000 nanoseconds
sparsity Attention out: 0.9706217447916666
[Time] - Reshape Time-Space: 19947.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9801432291666666
sparsity Attention postReshape chunk - 1 output: 0.9788411458333334
sparsity Attention postReshape chunk - 2 output: 0.9567057291666666
sparsity Attention postReshape chunk - 3 output: 0.966796875
[Time] - Transpose and LIF: 352033.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6537272135416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9679972330729166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
sparsity x_for_qkv: 0.6513671875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9668782552083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9683430989583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9673665364583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9668782552083334
sparsity Attention k_chunks: 0.9683430989583334
sparsity Attention v_chunks: 0.9673665364583334
[Time] - Reshape: 252979.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9668782552083334
sparsity Attention postReshape k_chunks: 0.9683430989583334
sparsity Attention postReshape v_chunks: 0.9673665364583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 105964.0000 nanoseconds
sparsity Attention out: 0.9777018229166666
[Time] - Reshape Time-Space: 20527.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9830729166666666
sparsity Attention postReshape chunk - 1 output: 0.9765625
sparsity Attention postReshape chunk - 2 output: 0.974609375
sparsity Attention postReshape chunk - 3 output: 0.9765625
[Time] - Transpose and LIF: 369566.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9996744791666666
sparsity Attention postReshape chunk - 1 x: 0.998046875
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6510416666666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9671834309895834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6513671875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9701334635416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.965576171875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9672037760416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9701334635416666
sparsity Attention k_chunks: 0.965576171875
sparsity Attention v_chunks: 0.9672037760416666
[Time] - Reshape: 256193.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9701334635416666
sparsity Attention postReshape k_chunks: 0.965576171875
sparsity Attention postReshape v_chunks: 0.9672037760416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 267665.0000 nanoseconds
sparsity Attention out: 0.974609375
[Time] - Reshape Time-Space: 26562.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9837239583333334
sparsity Attention postReshape chunk - 1 output: 0.9775390625
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9651692708333334
[Time] - Transpose and LIF: 370536.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6512858072916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9678548177083334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.650634765625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96826171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9711100260416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9681803385416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96826171875
sparsity Attention k_chunks: 0.9711100260416666
sparsity Attention v_chunks: 0.9681803385416666
[Time] - Reshape: 288954.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96826171875
sparsity Attention postReshape k_chunks: 0.9711100260416666
sparsity Attention postReshape v_chunks: 0.9681803385416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 255388.0000 nanoseconds
sparsity Attention out: 0.98193359375
[Time] - Reshape Time-Space: 27451.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9791666666666666
sparsity Attention postReshape chunk - 1 output: 0.9866536458333334
sparsity Attention postReshape chunk - 2 output: 0.9798177083333334
sparsity Attention postReshape chunk - 3 output: 0.9820963541666666
[Time] - Transpose and LIF: 348472.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6471354166666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9674886067708334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0023 seconds
[Time] - Spikeformer Time time: 0.0524 seconds
Run 23 - Tempo totale del modello: 0.0524 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.67919921875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9697265625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968505859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9702962239583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9697265625
sparsity Attention k_chunks: 0.968505859375
sparsity Attention v_chunks: 0.9702962239583334
[Time] - Reshape: 257152.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9697265625
sparsity Attention postReshape k_chunks: 0.968505859375
sparsity Attention postReshape v_chunks: 0.9702962239583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107813.0000 nanoseconds
sparsity Attention out: 0.9800618489583334
[Time] - Reshape Time-Space: 21846.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9908854166666666
sparsity Attention postReshape chunk - 1 output: 0.9820963541666666
sparsity Attention postReshape chunk - 2 output: 0.9775390625
sparsity Attention postReshape chunk - 3 output: 0.9697265625
[Time] - Transpose and LIF: 351105.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9951171875
sparsity Attention postReshape chunk - 3 x: 0.9944661458333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6776529947916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9701131184895834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.669921875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9683430989583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9697265625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9697265625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9683430989583334
sparsity Attention k_chunks: 0.9697265625
sparsity Attention v_chunks: 0.9697265625
[Time] - Reshape: 253165.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9683430989583334
sparsity Attention postReshape k_chunks: 0.9697265625
sparsity Attention postReshape v_chunks: 0.9697265625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 112727.0000 nanoseconds
sparsity Attention out: 0.9767252604166666
[Time] - Reshape Time-Space: 20505.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9853515625
sparsity Attention postReshape chunk - 1 output: 0.9827473958333334
sparsity Attention postReshape chunk - 2 output: 0.9661458333333334
sparsity Attention postReshape chunk - 3 output: 0.97265625
[Time] - Transpose and LIF: 354551.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6742350260416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9682210286458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6660970052083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968017578125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9694010416666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.96923828125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968017578125
sparsity Attention k_chunks: 0.9694010416666666
sparsity Attention v_chunks: 0.96923828125
[Time] - Reshape: 256324.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.968017578125
sparsity Attention postReshape k_chunks: 0.9694010416666666
sparsity Attention postReshape v_chunks: 0.96923828125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 263638.0000 nanoseconds
sparsity Attention out: 0.9761555989583334
[Time] - Reshape Time-Space: 26612.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9876302083333334
sparsity Attention postReshape chunk - 1 output: 0.9866536458333334
sparsity Attention postReshape chunk - 2 output: 0.96484375
sparsity Attention postReshape chunk - 3 output: 0.9654947916666666
[Time] - Transpose and LIF: 373140.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.670654296875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9687093098958334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.66162109375
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9674479166666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9706217447916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9720052083333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9674479166666666
sparsity Attention k_chunks: 0.9706217447916666
sparsity Attention v_chunks: 0.9720052083333334
[Time] - Reshape: 276960.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9674479166666666
sparsity Attention postReshape k_chunks: 0.9706217447916666
sparsity Attention postReshape v_chunks: 0.9720052083333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 259613.0000 nanoseconds
sparsity Attention out: 0.9795735677083334
[Time] - Reshape Time-Space: 27352.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9889322916666666
sparsity Attention postReshape chunk - 1 output: 0.9833984375
sparsity Attention postReshape chunk - 2 output: 0.9723307291666666
sparsity Attention postReshape chunk - 3 output: 0.9736328125
[Time] - Transpose and LIF: 370114.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6612955729166667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96905517578125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6597493489583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9694010416666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96875
sparsity Attention k_chunks: 0.9694010416666666
sparsity Attention v_chunks: 0.969970703125
[Time] - Reshape: 307766.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96875
sparsity Attention postReshape k_chunks: 0.9694010416666666
sparsity Attention postReshape v_chunks: 0.969970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 114856.0000 nanoseconds
sparsity Attention out: 0.9781087239583334
[Time] - Reshape Time-Space: 20310.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9781901041666666
sparsity Attention postReshape chunk - 1 output: 0.9694010416666666
sparsity Attention postReshape chunk - 2 output: 0.9833984375
sparsity Attention postReshape chunk - 3 output: 0.9814453125
[Time] - Transpose and LIF: 351272.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6559244791666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9664510091145834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6534830729166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9674479166666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966064453125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9694010416666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9674479166666666
sparsity Attention k_chunks: 0.966064453125
sparsity Attention v_chunks: 0.9694010416666666
[Time] - Reshape: 258594.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9674479166666666
sparsity Attention postReshape k_chunks: 0.966064453125
sparsity Attention postReshape v_chunks: 0.9694010416666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 269244.0000 nanoseconds
sparsity Attention out: 0.9689127604166666
[Time] - Reshape Time-Space: 28786.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9700520833333334
sparsity Attention postReshape chunk - 1 output: 0.9651692708333334
sparsity Attention postReshape chunk - 2 output: 0.9720052083333334
sparsity Attention postReshape chunk - 3 output: 0.9684244791666666
[Time] - Transpose and LIF: 360380.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.99609375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0046 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.652099609375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9687296549479166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6499837239583333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968017578125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676106770833334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9671223958333334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968017578125
sparsity Attention k_chunks: 0.9676106770833334
sparsity Attention v_chunks: 0.9671223958333334
[Time] - Reshape: 256042.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.968017578125
sparsity Attention postReshape k_chunks: 0.9676106770833334
sparsity Attention postReshape v_chunks: 0.9671223958333334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 265311.0000 nanoseconds
sparsity Attention out: 0.9742838541666666
[Time] - Reshape Time-Space: 27414.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9742838541666666
sparsity Attention postReshape chunk - 1 output: 0.9697265625
sparsity Attention postReshape chunk - 2 output: 0.9684244791666666
sparsity Attention postReshape chunk - 3 output: 0.9847005208333334
[Time] - Transpose and LIF: 367262.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9990234375
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6490885416666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672037760416666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6444498697916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968994140625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.967529296875
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9673665364583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968994140625
sparsity Attention k_chunks: 0.967529296875
sparsity Attention v_chunks: 0.9673665364583334
[Time] - Reshape: 282186.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.968994140625
sparsity Attention postReshape k_chunks: 0.967529296875
sparsity Attention postReshape v_chunks: 0.9673665364583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 105891.0000 nanoseconds
sparsity Attention out: 0.977294921875
[Time] - Reshape Time-Space: 20205.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9801432291666666
sparsity Attention postReshape chunk - 1 output: 0.984375
sparsity Attention postReshape chunk - 2 output: 0.9755859375
sparsity Attention postReshape chunk - 3 output: 0.9690755208333334
[Time] - Transpose and LIF: 358378.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.9977213541666666
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0039 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6451009114583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9660237630208334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
[Time] - Spikeformer Time time: 0.0533 seconds
Run 24 - Tempo totale del modello: 0.0533 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6761067708333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9715983072916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9706217447916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9701334635416666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9715983072916666
sparsity Attention k_chunks: 0.9706217447916666
sparsity Attention v_chunks: 0.9701334635416666
[Time] - Reshape: 305809.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9715983072916666
sparsity Attention postReshape k_chunks: 0.9706217447916666
sparsity Attention postReshape v_chunks: 0.9701334635416666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 106100.0000 nanoseconds
sparsity Attention out: 0.9803873697916666
[Time] - Reshape Time-Space: 20971.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9827473958333334
sparsity Attention postReshape chunk - 1 output: 0.9794921875
sparsity Attention postReshape chunk - 2 output: 0.98046875
sparsity Attention postReshape chunk - 3 output: 0.9788411458333334
[Time] - Transpose and LIF: 374405.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6773274739583333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9705810546875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.66796875
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9669596354166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9667154947916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9664713541666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9669596354166666
sparsity Attention k_chunks: 0.9667154947916666
sparsity Attention v_chunks: 0.9664713541666666
[Time] - Reshape: 310203.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9669596354166666
sparsity Attention postReshape k_chunks: 0.9667154947916666
sparsity Attention postReshape v_chunks: 0.9664713541666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 280185.0000 nanoseconds
sparsity Attention out: 0.97412109375
[Time] - Reshape Time-Space: 28080.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9775390625
sparsity Attention postReshape chunk - 1 output: 0.9680989583333334
sparsity Attention postReshape chunk - 2 output: 0.9733072916666666
sparsity Attention postReshape chunk - 3 output: 0.9775390625
[Time] - Transpose and LIF: 379563.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9964192708333334
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6629231770833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9687906901041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6614583333333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968994140625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96728515625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.968017578125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968994140625
sparsity Attention k_chunks: 0.96728515625
sparsity Attention v_chunks: 0.968017578125
[Time] - Reshape: 254582.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.968994140625
sparsity Attention postReshape k_chunks: 0.96728515625
sparsity Attention postReshape v_chunks: 0.968017578125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 259330.0000 nanoseconds
sparsity Attention out: 0.9752604166666666
[Time] - Reshape Time-Space: 26176.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9788411458333334
sparsity Attention postReshape chunk - 1 output: 0.9716796875
sparsity Attention postReshape chunk - 2 output: 0.9775390625
sparsity Attention postReshape chunk - 3 output: 0.9729817708333334
[Time] - Transpose and LIF: 356845.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9944661458333334
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.65380859375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672444661458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6494954427083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9686686197916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.966552734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9695638020833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9686686197916666
sparsity Attention k_chunks: 0.966552734375
sparsity Attention v_chunks: 0.9695638020833334
[Time] - Reshape: 307782.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9686686197916666
sparsity Attention postReshape k_chunks: 0.966552734375
sparsity Attention postReshape v_chunks: 0.9695638020833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107394.0000 nanoseconds
sparsity Attention out: 0.9807942708333334
[Time] - Reshape Time-Space: 21335.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9837239583333334
sparsity Attention postReshape chunk - 1 output: 0.9768880208333334
sparsity Attention postReshape chunk - 2 output: 0.9801432291666666
sparsity Attention postReshape chunk - 3 output: 0.982421875
[Time] - Transpose and LIF: 355930.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6524251302083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9666951497395834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6524251302083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9693196614583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9684244791666666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9679361979166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9693196614583334
sparsity Attention k_chunks: 0.9684244791666666
sparsity Attention v_chunks: 0.9679361979166666
[Time] - Reshape: 254510.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9693196614583334
sparsity Attention postReshape k_chunks: 0.9684244791666666
sparsity Attention postReshape v_chunks: 0.9679361979166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 261516.0000 nanoseconds
sparsity Attention out: 0.9781901041666666
[Time] - Reshape Time-Space: 27951.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9775390625
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.9710286458333334
sparsity Attention postReshape chunk - 3 output: 0.9840494791666666
[Time] - Transpose and LIF: 374070.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.9986979166666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6461588541666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676513671875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6424967447916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9647623697916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.96923828125
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9666341145833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9647623697916666
sparsity Attention k_chunks: 0.96923828125
sparsity Attention v_chunks: 0.9666341145833334
[Time] - Reshape: 252821.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9647623697916666
sparsity Attention postReshape k_chunks: 0.96923828125
sparsity Attention postReshape v_chunks: 0.9666341145833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 274649.0000 nanoseconds
sparsity Attention out: 0.97705078125
[Time] - Reshape Time-Space: 27868.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9788411458333334
sparsity Attention postReshape chunk - 1 output: 0.9798177083333334
sparsity Attention postReshape chunk - 2 output: 0.9716796875
sparsity Attention postReshape chunk - 3 output: 0.9778645833333334
[Time] - Transpose and LIF: 356773.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9990234375
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.64306640625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9676920572916666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6378580729166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9661458333333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9672037760416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9656575520833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9661458333333334
sparsity Attention k_chunks: 0.9672037760416666
sparsity Attention v_chunks: 0.9656575520833334
[Time] - Reshape: 283793.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9661458333333334
sparsity Attention postReshape k_chunks: 0.9672037760416666
sparsity Attention postReshape v_chunks: 0.9656575520833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 108014.0000 nanoseconds
sparsity Attention out: 0.972412109375
[Time] - Reshape Time-Space: 20665.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9781901041666666
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.9739583333333334
sparsity Attention postReshape chunk - 3 output: 0.9573567708333334
[Time] - Transpose and LIF: 360151.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9967447916666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6373697916666667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9670613606770834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6341959635416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.965576171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9681803385416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9666341145833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.965576171875
sparsity Attention k_chunks: 0.9681803385416666
sparsity Attention v_chunks: 0.9666341145833334
[Time] - Reshape: 350114.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.965576171875
sparsity Attention postReshape k_chunks: 0.9681803385416666
sparsity Attention postReshape v_chunks: 0.9666341145833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 106612.0000 nanoseconds
sparsity Attention out: 0.9691569010416666
[Time] - Reshape Time-Space: 20627.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9733072916666666
sparsity Attention postReshape chunk - 1 output: 0.9547526041666666
sparsity Attention postReshape chunk - 2 output: 0.982421875
sparsity Attention postReshape chunk - 3 output: 0.9661458333333334
[Time] - Transpose and LIF: 373249.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.99609375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.635009765625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9664510091145834
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
[Time] - Spikeformer Time time: 0.0527 seconds
Run 25 - Tempo totale del modello: 0.0527 secondi
[Time] - SPS.PSM: 0.0017 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6735026041666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.969970703125
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9710286458333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9698079427083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.969970703125
sparsity Attention k_chunks: 0.9710286458333334
sparsity Attention v_chunks: 0.9698079427083334
[Time] - Reshape: 253963.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.969970703125
sparsity Attention postReshape k_chunks: 0.9710286458333334
sparsity Attention postReshape v_chunks: 0.9698079427083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 273567.0000 nanoseconds
sparsity Attention out: 0.9833984375
[Time] - Reshape Time-Space: 28092.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9876302083333334
sparsity Attention postReshape chunk - 1 output: 0.9912109375
sparsity Attention postReshape chunk - 2 output: 0.9811197916666666
sparsity Attention postReshape chunk - 3 output: 0.9736328125
[Time] - Transpose and LIF: 374870.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9983723958333334
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6814778645833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9709269205729166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6700032552083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9684244791666666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968505859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.97021484375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9684244791666666
sparsity Attention k_chunks: 0.968505859375
sparsity Attention v_chunks: 0.97021484375
[Time] - Reshape: 276814.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9684244791666666
sparsity Attention postReshape k_chunks: 0.968505859375
sparsity Attention postReshape v_chunks: 0.97021484375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 253336.0000 nanoseconds
sparsity Attention out: 0.9746907552083334
[Time] - Reshape Time-Space: 27288.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.98046875
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.974609375
sparsity Attention postReshape chunk - 3 output: 0.9632161458333334
[Time] - Transpose and LIF: 362046.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9983723958333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6660970052083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9712727864583334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6582845052083333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9681803385416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968505859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.969970703125
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9681803385416666
sparsity Attention k_chunks: 0.968505859375
sparsity Attention v_chunks: 0.969970703125
[Time] - Reshape: 304693.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9681803385416666
sparsity Attention postReshape k_chunks: 0.968505859375
sparsity Attention postReshape v_chunks: 0.969970703125
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 106221.0000 nanoseconds
sparsity Attention out: 0.9742024739583334
[Time] - Reshape Time-Space: 19095.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9807942708333334
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.9700520833333334
sparsity Attention postReshape chunk - 3 output: 0.9658203125
[Time] - Transpose and LIF: 346858.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9996744791666666
sparsity Attention postReshape chunk - 3 x: 0.998046875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6555989583333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96942138671875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6520182291666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9685872395833334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9683430989583334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9659016927083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9685872395833334
sparsity Attention k_chunks: 0.9683430989583334
sparsity Attention v_chunks: 0.9659016927083334
[Time] - Reshape: 254597.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9685872395833334
sparsity Attention postReshape k_chunks: 0.9683430989583334
sparsity Attention postReshape v_chunks: 0.9659016927083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 280515.0000 nanoseconds
sparsity Attention out: 0.9717610677083334
[Time] - Reshape Time-Space: 29392.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9853515625
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.9602864583333334
sparsity Attention postReshape chunk - 3 output: 0.9609375
[Time] - Transpose and LIF: 376787.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9977213541666666
sparsity Attention postReshape chunk - 3 x: 0.994140625
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.649658203125
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96868896484375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6448567708333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9706217447916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9680989583333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9684244791666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9706217447916666
sparsity Attention k_chunks: 0.9680989583333334
sparsity Attention v_chunks: 0.9684244791666666
[Time] - Reshape: 277577.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9706217447916666
sparsity Attention postReshape k_chunks: 0.9680989583333334
sparsity Attention postReshape v_chunks: 0.9684244791666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 278678.0000 nanoseconds
sparsity Attention out: 0.97802734375
[Time] - Reshape Time-Space: 26963.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9762369791666666
sparsity Attention postReshape chunk - 1 output: 0.9798177083333334
sparsity Attention postReshape chunk - 2 output: 0.9798177083333334
sparsity Attention postReshape chunk - 3 output: 0.9762369791666666
[Time] - Transpose and LIF: 365273.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9977213541666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6444498697916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96759033203125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.642822265625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9673665364583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9669596354166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.963623046875
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9673665364583334
sparsity Attention k_chunks: 0.9669596354166666
sparsity Attention v_chunks: 0.963623046875
[Time] - Reshape: 259107.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9673665364583334
sparsity Attention postReshape k_chunks: 0.9669596354166666
sparsity Attention postReshape v_chunks: 0.963623046875
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 106843.0000 nanoseconds
sparsity Attention out: 0.9737141927083334
[Time] - Reshape Time-Space: 20294.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9733072916666666
sparsity Attention postReshape chunk - 1 output: 0.9710286458333334
sparsity Attention postReshape chunk - 2 output: 0.9788411458333334
sparsity Attention postReshape chunk - 3 output: 0.9716796875
[Time] - Transpose and LIF: 364801.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.9986979166666666
sparsity Attention postReshape chunk - 2 x: 0.9993489583333334
sparsity Attention postReshape chunk - 3 x: 0.9951171875
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6394856770833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9677530924479166
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6399739583333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9680989583333334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9671223958333334
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9677734375
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9680989583333334
sparsity Attention k_chunks: 0.9671223958333334
sparsity Attention v_chunks: 0.9677734375
[Time] - Reshape: 252807.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9680989583333334
sparsity Attention postReshape k_chunks: 0.9671223958333334
sparsity Attention postReshape v_chunks: 0.9677734375
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 274217.0000 nanoseconds
sparsity Attention out: 0.9752604166666666
[Time] - Reshape Time-Space: 28034.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9827473958333334
sparsity Attention postReshape chunk - 1 output: 0.9781901041666666
sparsity Attention postReshape chunk - 2 output: 0.9700520833333334
sparsity Attention postReshape chunk - 3 output: 0.9700520833333334
[Time] - Transpose and LIF: 372864.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9973958333333334
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9951171875
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6381022135416667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9686279296875
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.63916015625
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.968994140625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9676920572916666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9659830729166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.968994140625
sparsity Attention k_chunks: 0.9676920572916666
sparsity Attention v_chunks: 0.9659830729166666
[Time] - Reshape: 256706.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.968994140625
sparsity Attention postReshape k_chunks: 0.9676920572916666
sparsity Attention postReshape v_chunks: 0.9659830729166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 262869.0000 nanoseconds
sparsity Attention out: 0.9752604166666666
[Time] - Reshape Time-Space: 26419.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9772135416666666
sparsity Attention postReshape chunk - 1 output: 0.9801432291666666
sparsity Attention postReshape chunk - 2 output: 0.9716796875
sparsity Attention postReshape chunk - 3 output: 0.9720052083333334
[Time] - Transpose and LIF: 354835.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9993489583333334
sparsity Attention postReshape chunk - 2 x: 0.9973958333333334
sparsity Attention postReshape chunk - 3 x: 0.9957682291666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.636474609375
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9669189453125
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
[Time] - Spikeformer Time time: 0.0530 seconds
Run 26 - Tempo totale del modello: 0.0531 secondi
[Time] - SPS.PSM: 0.0018 seconds
[Time] - SPS.RPE: 0.0004 seconds
sparsity x_for_qkv: 0.6819661458333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.970947265625
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9698893229166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9703776041666666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.970947265625
sparsity Attention k_chunks: 0.9698893229166666
sparsity Attention v_chunks: 0.9703776041666666
[Time] - Reshape: 258705.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.970947265625
sparsity Attention postReshape k_chunks: 0.9698893229166666
sparsity Attention postReshape v_chunks: 0.9703776041666666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 257536.0000 nanoseconds
sparsity Attention out: 0.9832356770833334
[Time] - Reshape Time-Space: 27549.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9934895833333334
sparsity Attention postReshape chunk - 1 output: 0.9925130208333334
sparsity Attention postReshape chunk - 2 output: 0.9733072916666666
sparsity Attention postReshape chunk - 3 output: 0.9736328125
[Time] - Transpose and LIF: 371227.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9970703125
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6829427083333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9694417317708334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6722005208333333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9667154947916666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968994140625
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9685872395833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9667154947916666
sparsity Attention k_chunks: 0.968994140625
sparsity Attention v_chunks: 0.9685872395833334
[Time] - Reshape: 315701.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9667154947916666
sparsity Attention postReshape k_chunks: 0.968994140625
sparsity Attention postReshape v_chunks: 0.9685872395833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107570.0000 nanoseconds
sparsity Attention out: 0.9736328125
[Time] - Reshape Time-Space: 25133.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9866536458333334
sparsity Attention postReshape chunk - 1 output: 0.9723307291666666
sparsity Attention postReshape chunk - 2 output: 0.96484375
sparsity Attention postReshape chunk - 3 output: 0.970703125
[Time] - Transpose and LIF: 359070.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.998046875
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9990234375
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6690266927083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9683837890625
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6588541666666667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.967529296875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968505859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9676106770833334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.967529296875
sparsity Attention k_chunks: 0.968505859375
sparsity Attention v_chunks: 0.9676106770833334
[Time] - Reshape: 320457.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.967529296875
sparsity Attention postReshape k_chunks: 0.968505859375
sparsity Attention postReshape v_chunks: 0.9676106770833334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 265974.0000 nanoseconds
sparsity Attention out: 0.9756673177083334
[Time] - Reshape Time-Space: 32580.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9772135416666666
sparsity Attention postReshape chunk - 1 output: 0.9733072916666666
sparsity Attention postReshape chunk - 2 output: 0.9736328125
sparsity Attention postReshape chunk - 3 output: 0.978515625
[Time] - Transpose and LIF: 357447.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9983723958333334
sparsity Attention postReshape chunk - 1 x: 0.99609375
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0042 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.666259765625
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9681396484375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6630045572916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9678548177083334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9669596354166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9688313802083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9678548177083334
sparsity Attention k_chunks: 0.9669596354166666
sparsity Attention v_chunks: 0.9688313802083334
[Time] - Reshape: 275128.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9678548177083334
sparsity Attention postReshape k_chunks: 0.9669596354166666
sparsity Attention postReshape v_chunks: 0.9688313802083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 308169.0000 nanoseconds
sparsity Attention out: 0.973876953125
[Time] - Reshape Time-Space: 28431.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9677734375
sparsity Attention postReshape chunk - 1 output: 0.9772135416666666
sparsity Attention postReshape chunk - 2 output: 0.9723307291666666
sparsity Attention postReshape chunk - 3 output: 0.9781901041666666
[Time] - Transpose and LIF: 376305.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9993489583333334
sparsity Attention postReshape chunk - 1 x: 0.9983723958333334
sparsity Attention postReshape chunk - 2 x: 0.9947916666666666
sparsity Attention postReshape chunk - 3 x: 0.9996744791666666
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0002 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.664794921875
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9682210286458334
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0019 seconds
sparsity x_for_qkv: 0.6544596354166667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.96826171875
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9669596354166666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.967041015625
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.96826171875
sparsity Attention k_chunks: 0.9669596354166666
sparsity Attention v_chunks: 0.967041015625
[Time] - Reshape: 254752.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.96826171875
sparsity Attention postReshape k_chunks: 0.9669596354166666
sparsity Attention postReshape v_chunks: 0.967041015625
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107125.0000 nanoseconds
sparsity Attention out: 0.9757486979166666
[Time] - Reshape Time-Space: 20708.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9817708333333334
sparsity Attention postReshape chunk - 1 output: 0.9775390625
sparsity Attention postReshape chunk - 2 output: 0.9684244791666666
sparsity Attention postReshape chunk - 3 output: 0.9752604166666666
[Time] - Transpose and LIF: 362824.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 0.9967447916666666
sparsity Attention postReshape chunk - 3 x: 0.9973958333333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6571451822916667
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9672037760416666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
sparsity x_for_qkv: 0.6561686197916667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9654134114583334
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9672037760416666
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9679361979166666
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9654134114583334
sparsity Attention k_chunks: 0.9672037760416666
sparsity Attention v_chunks: 0.9679361979166666
[Time] - Reshape: 313669.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9654134114583334
sparsity Attention postReshape k_chunks: 0.9672037760416666
sparsity Attention postReshape v_chunks: 0.9679361979166666
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 107129.0000 nanoseconds
sparsity Attention out: 0.976318359375
[Time] - Reshape Time-Space: 21632.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9847005208333334
sparsity Attention postReshape chunk - 1 output: 0.98046875
sparsity Attention postReshape chunk - 2 output: 0.9700520833333334
sparsity Attention postReshape chunk - 3 output: 0.9700520833333334
[Time] - Transpose and LIF: 348730.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9977213541666666
sparsity Attention postReshape chunk - 1 x: 0.9970703125
sparsity Attention postReshape chunk - 2 x: 0.9990234375
sparsity Attention postReshape chunk - 3 x: 0.9993489583333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0040 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6565755208333333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9673258463541666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0021 seconds
sparsity x_for_qkv: 0.6521809895833333
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9679361979166666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.968505859375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9668782552083334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9679361979166666
sparsity Attention k_chunks: 0.968505859375
sparsity Attention v_chunks: 0.9668782552083334
[Time] - Reshape: 351647.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9679361979166666
sparsity Attention postReshape k_chunks: 0.968505859375
sparsity Attention postReshape v_chunks: 0.9668782552083334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 258708.0000 nanoseconds
sparsity Attention out: 0.978271484375
[Time] - Reshape Time-Space: 26397.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9833984375
sparsity Attention postReshape chunk - 1 output: 0.9694010416666666
sparsity Attention postReshape chunk - 2 output: 0.9817708333333334
sparsity Attention postReshape chunk - 3 output: 0.978515625
[Time] - Transpose and LIF: 448076.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 1.0
sparsity Attention postReshape chunk - 1 x: 1.0
sparsity Attention postReshape chunk - 2 x: 1.0
sparsity Attention postReshape chunk - 3 x: 1.0
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0043 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6521809895833333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.96630859375
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0020 seconds
sparsity x_for_qkv: 0.6527506510416667
shape x_for_qkv: torch.Size([4, 768, 2, 2])
sparsity q: 0.9662272135416666
q shape: torch.Size([4, 1, 8, 4, 96])
sparsity k: 0.9677734375
k shape: torch.Size([4, 1, 8, 4, 96])
sparsity v: 0.9673665364583334
v shape: torch.Size([4, 1, 8, 4, 96])
sparsity Attention q_chunks: 0.9662272135416666
sparsity Attention k_chunks: 0.9677734375
sparsity Attention v_chunks: 0.9673665364583334
[Time] - Reshape: 315698.0000 nanoseconds
sparsity Attention postReshape q_chunks: 0.9662272135416666
sparsity Attention postReshape k_chunks: 0.9677734375
sparsity Attention postReshape v_chunks: 0.9673665364583334
q_chunks shape: torch.Size([2, 1, 8, 8, 96])
k_chunks shape: torch.Size([2, 1, 8, 8, 96])
v_chunks shape: torch.Size([2, 1, 8, 8, 96])
[Time] - Matmul: 252706.0000 nanoseconds
sparsity Attention out: 0.9737955729166666
[Time] - Reshape Time-Space: 30526.0000 nanoseconds
shape : torch.Size([4, 1, 8, 4, 96])
sparsity Attention postReshape chunk - 0 output: 0.9765625
sparsity Attention postReshape chunk - 1 output: 0.9772135416666666
sparsity Attention postReshape chunk - 2 output: 0.9710286458333334
sparsity Attention postReshape chunk - 3 output: 0.9703776041666666
[Time] - Transpose and LIF: 371594.0000 nanoseconds
sparsity Attention postReshape chunk - 0 x: 0.9986979166666666
sparsity Attention postReshape chunk - 1 x: 0.9996744791666666
sparsity Attention postReshape chunk - 2 x: 0.9986979166666666
sparsity Attention postReshape chunk - 3 x: 0.9934895833333334
x shape: torch.Size([4, 1, 768, 2, 2])
[Time] - Projection and BN: 0.0001 seconds
x shape after proj: torch.Size([4, 1, 768, 2, 2])
sparsity x: 0.0
[Time] - SSA Encoder: 0.0041 seconds
MLP sparsity x: 0.0
MLP sparsity postLIF x after lif: 0.6494954427083333
MLP sparsity postBN x after fc1: 0.0
MLP sparsity postLIF x after fc2: 0.9678141276041666
MLP sparsity postBN x after fc2ConvBn: 0.0
MLP sparsity x after identity final: 0.0
[Time] - MLP: 0.0022 seconds
[Time] - Spikeformer Time time: 0.0525 seconds
Run 27 - Tempo totale del modello: 0.0525 secondi
